{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Aneurysm Detection Training - exp0001\n",
    "\n",
    "**実験概要**: ResNet50 ベースラインモデル\n",
    "\n",
    "**実行環境**: \n",
    "- Google Colab (GPU)\n",
    "- PyTorch with Mixed Precision\n",
    "- W&B Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# GPU確認\n",
    "!nvidia-smi\n",
    "\n",
    "# リポジトリクローン（初回のみ）\n",
    "repo_name = \"DATASCIENCE-TEM\"\n",
    "repo_url = \"https://github.com/YOUR_USERNAME/DATASCIENCE-TEM.git\"\n",
    "\n",
    "if not os.path.exists(f\"/content/{repo_name}\"):\n",
    "    print(f\"Cloning {repo_url}...\")\n",
    "    !git clone {repo_url}\n",
    "else:\n",
    "    print(f\"Repository {repo_name} already exists, updating...\")\n",
    "    %cd /content/{repo_name}\n",
    "    !git pull origin main\n",
    "\n",
    "%cd /content/DATASCIENCE-TEM/kaggle-projects/rsna-aneurysm/experiments/exp0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係インストール\n",
    "!pip install -r env/requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API認証設定\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
    "\n",
    "# Google Drive マウント（成果物保存用）\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle CLI 用の認証ファイル作成（環境変数から生成）\n",
    "import os, json, pathlib, subprocess\n",
    "kdir = pathlib.Path.home().joinpath('.kaggle')\n",
    "kdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(kdir.joinpath('kaggle.json'), 'w') as f:\n",
    "    f.write(json.dumps({'username': os.environ.get('KAGGLE_USERNAME', ''), 'key': os.environ.get('KAGGLE_KEY', '')}))\n",
    "kjson = kdir.joinpath('kaggle.json')\n",
    "try:\n",
    "    subprocess.run(['chmod', '600', str(kjson)], check=False)\n",
    "except Exception as e:\n",
    "    print('chmod failed:', e)\n",
    "print('Kaggle credentials prepared at ~/.kaggle/kaggle.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ準備（Kaggleダウンロード→メタ抽出→PNG変換→学習用メタ生成）\n",
    "from pathlib import Path\n",
    "import os, subprocess, shlex\n",
    "\n",
    "# 作業パス確認（このノートは exp0001 直下で動かす想定）\n",
    "print('CWD:', os.getcwd())\n",
    "\n",
    "# ルート相対パス\n",
    "RAW_DIR = Path('../../data/raw')\n",
    "PROC_DIR = Path('../../data/processed')\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Kaggle データダウンロード（存在すればスキップ）\n",
    "if not (RAW_DIR / 'train.csv').exists():\n",
    "    cmd = 'python ../../scripts/download_data.py --competition rsna-intracranial-aneurysm-detection --output ../../data/raw'\n",
    "    subprocess.run(shlex.split(cmd), check=True)\n",
    "else:\n",
    "    print('Skip download: train.csv found')\n",
    "\n",
    "# 2) DICOM メタデータ抽出（存在すればスキップ）\n",
    "if not (PROC_DIR / 'train_metadata.csv').exists():\n",
    "    cmd = 'python ../../scripts/dicom_utils.py extract-metadata --input ../../data/raw/train_images --output ../../data/processed/train_metadata.csv'\n",
    "    subprocess.run(shlex.split(cmd), check=True)\n",
    "else:\n",
    "    print('Skip metadata extract: train_metadata.csv found')\n",
    "\n",
    "# 3) DICOM → PNG 変換（PNGが未生成なら実行）\n",
    "# 画像は processed 直下に <SOPInstanceUID>.png で出力（Datasetがそのまま参照）\n",
    "if not any(PROC_DIR.glob('*.png')):\n",
    "    cmd = 'python ../../scripts/dicom_utils.py convert-images --input ../../data/raw/train_images --output ../../data/processed --format png --target-size 512 512 --window-center 40 --window-width 80'\n",
    "    subprocess.run(shlex.split(cmd), check=True)\n",
    "else:\n",
    "    print('Skip convert: PNG already exists in processed dir')\n",
    "\n",
    "# 4) 学習用メタ生成（ノートブックは processed/train.csv を読む想定）\n",
    "if not (PROC_DIR / 'train.csv').exists():\n",
    "    cmd = 'python ../../scripts/create_metadata.py --train-csv ../../data/raw/train.csv --metadata ../../data/processed/train_metadata.csv --output ../../data/processed/train_processed.csv'\n",
    "    subprocess.run(shlex.split(cmd), check=True)\n",
    "    # cp の代わりにPythonでコピー\n",
    "    import shutil\n",
    "    shutil.copy2('../../data/processed/train_processed.csv', '../../data/processed/train.csv')\n",
    "else:\n",
    "    print('Skip create_metadata: processed/train.csv found')\n",
    "\n",
    "print('Data preparation completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "# PyTorch関連\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 画像処理\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pydicom\n",
    "\n",
    "# ML・評価指標\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定読み込み\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Experiment ID: {cfg['experiment']['id']}\")\n",
    "print(f\"Description: {cfg['experiment']['description']}\")\n",
    "print(f\"Model: {cfg['model']['architecture']}\")\n",
    "print(f\"Image Size: {cfg['data']['image_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード設定（再現性確保）\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cfg[\"environment\"][\"deterministic\"]\n",
    "    torch.backends.cudnn.benchmark = not cfg[\"environment\"][\"deterministic\"]\n",
    "\n",
    "\n",
    "set_seed(cfg[\"environment\"][\"seed\"])\n",
    "device = torch.device(cfg[\"environment\"][\"device\"])\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git SHA取得（バージョン管理）\n",
    "def get_git_sha():\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "git_sha = get_git_sha()\n",
    "print(f\"Git SHA: {git_sha}\")\n",
    "\n",
    "# Git SHA保存\n",
    "with open(\"git_sha.txt\", \"w\") as f:\n",
    "    f.write(git_sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "data_dir = Path(cfg[\"paths\"][\"data_dir\"])\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# データファイルの存在確認\n",
    "train_csv_path = data_dir / \"processed\" / \"train.csv\"\n",
    "if not train_csv_path.exists():\n",
    "    print(f\"Warning: {train_csv_path} not found. Creating sample data for testing...\")\n",
    "    # サンプルデータ作成（実際のデータがない場合の対応）\n",
    "    sample_data = {\n",
    "        \"image_id\": [f\"img_{i:04d}\" for i in range(100)],\n",
    "        \"PatientID\": [f\"patient_{i // 10:03d}\" for i in range(100)],\n",
    "        cfg[\"data\"][\"target_column\"]: np.random.choice([0, 1], 100, p=[0.7, 0.3]),\n",
    "    }\n",
    "    sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    # 保存ディレクトリ作成\n",
    "    (data_dir / \"processed\").mkdir(parents=True, exist_ok=True)\n",
    "    sample_df.to_csv(train_csv_path, index=False)\n",
    "    print(f\"Sample data saved to {train_csv_path}\")\n",
    "\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df[cfg[\"data\"][\"target_column\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV分割作成\n",
    "sgkf = StratifiedGroupKFold(n_splits=cfg[\"cv\"][\"n_folds\"], shuffle=True, random_state=cfg[\"cv\"][\"seed\"])\n",
    "\n",
    "train_df[\"fold\"] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    sgkf.split(train_df, train_df[cfg[\"data\"][\"target_column\"]], train_df[cfg[\"cv\"][\"group_column\"]])\n",
    "):\n",
    "    train_df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "# CV分割保存\n",
    "train_df[[\"PatientID\", \"fold\", cfg[\"data\"][\"target_column\"]]].to_csv(\"cv_folds.csv\", index=False)\n",
    "print(\"CV splits saved to cv_folds.csv\")\n",
    "\n",
    "# Fold分布確認\n",
    "print(\"\\nFold distribution:\")\n",
    "print(train_df.groupby(\"fold\")[cfg[\"data\"][\"target_column\"]].value_counts().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張設定\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.Rotate(limit=10, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "                A.CLAHE(p=0.3),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset定義\n",
    "class AneurysmDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, mode=\"train\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        # 画像ディレクトリの存在確認\n",
    "        if not self.image_dir.exists():\n",
    "            print(f\"Warning: Image directory {self.image_dir} does not exist.\")\n",
    "            print(\"Creating dummy images for testing...\")\n",
    "            self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._create_dummy_images()\n",
    "\n",
    "    def _create_dummy_images(self):\n",
    "        \"\"\"テスト用のダミー画像を作成\"\"\"\n",
    "        for idx, row in self.df.iterrows():\n",
    "            image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "            if not image_path.exists():\n",
    "                # 512x512のランダム画像を作成\n",
    "                dummy_image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "                cv2.imwrite(str(image_path), dummy_image)\n",
    "        print(f\"Created {len(self.df)} dummy images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 画像読み込み\n",
    "        image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "\n",
    "        try:\n",
    "            image = cv2.imread(str(image_path))\n",
    "            if image is None:\n",
    "                raise FileNotFoundError(f\"Could not load image: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # フォールバック：ランダム画像\n",
    "            image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            label = torch.tensor(row[cfg[\"data\"][\"target_column\"]], dtype=torch.float)\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "class AneurysmModel(nn.Module):\n",
    "    def __init__(self, model_name=\"resnet50\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # バックボーン\n",
    "        if model_name == \"resnet50\":\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # 分類ヘッド\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(in_features, cfg[\"model\"][\"hidden_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(cfg[\"model\"][\"hidden_dim\"], num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# モデル初期化テスト\n",
    "model = AneurysmModel(\n",
    "    model_name=cfg[\"model\"][\"architecture\"], num_classes=cfg[\"model\"][\"num_classes\"], pretrained=cfg[\"model\"][\"pretrained\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Model created: {cfg['model']['architecture']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B初期化\n",
    "run = wandb.init(\n",
    "    project=cfg[\"logging\"][\"wandb\"][\"project\"],\n",
    "    name=cfg[\"logging\"][\"wandb\"][\"name\"],\n",
    "    config=cfg,\n",
    "    tags=cfg[\"logging\"][\"wandb\"][\"tags\"],\n",
    "    notes=cfg[\"logging\"][\"wandb\"][\"notes\"],\n",
    ")\n",
    "\n",
    "# W&B URL保存\n",
    "wandb_url = f\"https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\"\n",
    "with open(\"wandb_run.txt\", \"w\") as f:\n",
    "    f.write(f\"URL: {wandb_url}\\n\")\n",
    "    f.write(f\"Run ID: {wandb.run.id}\\n\")\n",
    "\n",
    "print(f\"W&B run: {wandb_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・評価関数\n",
    "def train_epoch(model, dataloader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(enabled=cfg[\"environment\"][\"mixed_precision\"]):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if cfg[\"environment\"][\"mixed_precision\"]:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            with autocast(enabled=cfg[\"environment\"][\"mixed_precision\"]):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 予測確率に変換\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            predictions.extend(probs.flatten())\n",
    "            targets.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    return avg_loss, auc, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(train_df))\n",
    "oof_targets = train_df[cfg[\"data\"][\"target_column\"]].values\n",
    "\n",
    "# モデル保存ディレクトリ作成\n",
    "Path(\"model\").mkdir(exist_ok=True)\n",
    "\n",
    "for fold in range(cfg[\"cv\"][\"n_folds\"]):\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"FOLD {fold + 1}/{cfg['cv']['n_folds']}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # データ分割\n",
    "    train_fold_df = train_df[train_df[\"fold\"] != fold]\n",
    "    valid_fold_df = train_df[train_df[\"fold\"] == fold]\n",
    "\n",
    "    print(f\"Train: {len(train_fold_df)}, Valid: {len(valid_fold_df)}\")\n",
    "\n",
    "    # Dataset・DataLoader作成\n",
    "    train_dataset = AneurysmDataset(train_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"train\"))\n",
    "    valid_dataset = AneurysmDataset(valid_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"valid\"))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,  # Colabでは2に制限\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=2,  # Colabでは2に制限\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # モデル・最適化設定\n",
    "    model = AneurysmModel(\n",
    "        model_name=cfg[\"model\"][\"architecture\"], num_classes=cfg[\"model\"][\"num_classes\"], pretrained=cfg[\"model\"][\"pretrained\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # 最適化設定の修正\n",
    "    optimizer_type = cfg[\"train\"][\"optimizer\"][\"type\"]\n",
    "    if optimizer_type == \"AdamW\":\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "    else:\n",
    "        # フォールバック\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=cfg[\"environment\"][\"mixed_precision\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=cfg[\"train\"][\"scheduler\"][\"T_0\"], eta_min=cfg[\"train\"][\"scheduler\"][\"eta_min\"]\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    best_auc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg[\"train\"][\"epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
    "        valid_loss, valid_auc, valid_preds, valid_targets = validate_epoch(model, valid_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{cfg['train']['epochs']}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid AUC: {valid_auc:.4f}\")\n",
    "\n",
    "        # W&Bログ\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"fold_{fold}_train_loss\": train_loss,\n",
    "                f\"fold_{fold}_valid_loss\": valid_loss,\n",
    "                f\"fold_{fold}_valid_auc\": valid_auc,\n",
    "                \"epoch\": epoch,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Best model保存\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"model/fold_{fold}_best.pth\")\n",
    "            best_preds = valid_preds.copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early Stopping\n",
    "        if patience_counter >= cfg[\"train\"][\"early_stopping\"][\"patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # OOF予測保存\n",
    "    valid_indices = valid_fold_df.index\n",
    "    oof_predictions[valid_indices] = best_preds\n",
    "\n",
    "    fold_scores.append(best_auc)\n",
    "    print(f\"Fold {fold + 1} Best AUC: {best_auc:.4f}\")\n",
    "\n",
    "    # メモリクリア\n",
    "    del model, optimizer, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# CV結果集計\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "oof_auc = roc_auc_score(oof_targets, oof_predictions)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"CV RESULTS\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Fold Scores: {fold_scores}\")\n",
    "print(f\"CV Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"OOF AUC: {oof_auc:.4f}\")\n",
    "\n",
    "# W&Bに最終結果ログ\n",
    "wandb.log({\"cv_mean_auc\": cv_mean, \"cv_std_auc\": cv_std, \"oof_auc\": oof_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF結果保存\n",
    "oof_df = pd.DataFrame({\"index\": train_df.index, \"fold\": train_df[\"fold\"], \"y_true\": oof_targets, \"y_pred\": oof_predictions})\n",
    "oof_df.to_csv(\"oof_predictions.csv\", index=False)\n",
    "\n",
    "# メトリクス保存\n",
    "metrics = {\n",
    "    \"experiment_id\": cfg[\"experiment\"][\"id\"],\n",
    "    \"cv_mean_auc\": float(cv_mean),\n",
    "    \"cv_std_auc\": float(cv_std),\n",
    "    \"oof_auc\": float(oof_auc),\n",
    "    \"fold_scores\": [float(x) for x in fold_scores],\n",
    "    \"git_sha\": git_sha,\n",
    "    \"wandb_url\": wandb_url,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(\"- oof_predictions.csv\")\n",
    "print(\"- metrics.json\")\n",
    "print(f\"- model/fold_*_best.pth ({cfg['cv']['n_folds']} files)\")\n",
    "print(\"- wandb_run.txt\")\n",
    "print(\"- git_sha.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive に成果物バックアップ\n",
    "backup_dir = f\"/content/drive/MyDrive/rsna-aneurysm/{cfg['experiment']['id']}\"\n",
    "!mkdir -p \"{backup_dir}\"\n",
    "!cp oof_predictions.csv \"{backup_dir}/\"\n",
    "!cp metrics.json \"{backup_dir}/\"\n",
    "!cp -r model \"{backup_dir}/\"\n",
    "!cp wandb_run.txt \"{backup_dir}/\"\n",
    "!cp git_sha.txt \"{backup_dir}/\"\n",
    "\n",
    "print(f\"Results backed up to: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B終了\n",
    "wandb.finish()\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
