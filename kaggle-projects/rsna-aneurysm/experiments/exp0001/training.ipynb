{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Aneurysm Detection Training - exp0001\n",
    "\n",
    "**実験概要**: ResNet50 ベースラインモデル\n",
    "\n",
    "**実行環境**: \n",
    "- Google Colab (GPU)\n",
    "- PyTorch with Mixed Precision\n",
    "- W&B Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# GPU確認\n",
    "!nvidia-smi\n",
    "\n",
    "# リポジトリクローン（初回のみ）\n",
    "repo_name = \"DATASCIENCE-TEM\"\n",
    "repo_url = os.environ.get(\"REPO_URL\", \"https://github.com/YOUR_USERNAME/DATASCIENCE-TEM.git\")\n",
    "\n",
    "if not os.path.exists(f\"/content/{repo_name}\"):\n",
    "    print(f\"Cloning {repo_url}...\")\n",
    "    !git clone {repo_url}\n",
    "    %cd /content/{repo_name}\n",
    "else:\n",
    "    print(f\"Repository {repo_name} already exists, updating...\")\n",
    "    %cd /content/{repo_name}\n",
    "    !git pull origin main || true\n",
    "\n",
    "# 実験ディレクトリへ移動（常に明示）\n",
    "%cd /content/DATASCIENCE-TEM/kaggle-projects/rsna-aneurysm/experiments/exp0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係インストール（Colab向け）\n",
    "# - PyTorch は cu121 ホイールを明示（Colab の CUDA12 系に整合）\n",
    "# - torch 以外の依存のみ個別に追加\n",
    "!pip -q install --upgrade pip\n",
    "\n",
    "# PyTorch 2.4.1 + cu121（torchvision/torchaudio も整合）\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n",
    "\n",
    "# プロジェクト依存（torch 以外）\n",
    "!pip -q install timm==1.0.16 albumentations==2.0.8 opencv-python-headless==4.10.0.84 \\\n",
    "  nibabel>=5.2.1 pydicom>=2.4.4 monai>=1.4.0 lightning==2.4.0 accelerate==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API認証設定\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
    "\n",
    "# Google Drive マウント（成果物保存用）\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle CLI 用の認証ファイル作成（環境変数から生成）\n",
    "import os, json, pathlib, subprocess\n",
    "kdir = pathlib.Path.home().joinpath('.kaggle')\n",
    "kdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(kdir.joinpath('kaggle.json'), 'w') as f:\n",
    "    f.write(json.dumps({'username': os.environ.get('KAGGLE_USERNAME', ''), 'key': os.environ.get('KAGGLE_KEY', '')}))\n",
    "kjson = kdir.joinpath('kaggle.json')\n",
    "try:\n",
    "    subprocess.run(['chmod', '600', str(kjson)], check=False)\n",
    "except Exception as e:\n",
    "    print('chmod failed:', e)\n",
    "print('Kaggle credentials prepared at ~/.kaggle/kaggle.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# データ準備（Kaggleダウンロード→メタ抽出→PNG変換→学習用メタ生成）\nfrom pathlib import Path\nimport os, sys, subprocess\n\n# 作業パス確認（このノートは exp0001 直下で動かす想定）\nprint('CWD:', os.getcwd())\n\n# Safe path detection for different environments (fixes Google Colab IndexError)\ncurrent_dir = Path.cwd()\nprint(f\"Current directory: {current_dir}\")\n\nif current_dir.name == 'content' or '/content' in str(current_dir):\n    # Google Colab environment - navigate to project root\n    print(\"Detected Google Colab environment\")\n    RSNA_DIR = current_dir / 'DATASCIENCE-TEM' / 'kaggle-projects' / 'rsna-aneurysm'\n    if not RSNA_DIR.exists():\n        # Alternative Colab path if cloned differently\n        RSNA_DIR = current_dir / 'kaggle-projects' / 'rsna-aneurysm'\nelif current_dir.name == 'exp0001':\n    # Local development environment - go up two levels\n    print(\"Detected local development environment\")\n    RSNA_DIR = current_dir.parents[1]\nelse:\n    # Fallback with safe path handling\n    print(f\"Unknown environment, using fallback logic\")\n    try:\n        RSNA_DIR = current_dir.parents[1] if len(current_dir.parents) > 1 else current_dir.parent\n    except IndexError:\n        # If parents[1] fails, assume we're in a restricted environment\n        RSNA_DIR = current_dir / 'DATASCIENCE-TEM' / 'kaggle-projects' / 'rsna-aneurysm'\n        if not RSNA_DIR.exists():\n            RSNA_DIR = current_dir / 'kaggle-projects' / 'rsna-aneurysm'\n\nprint(f\"RSNA project directory: {RSNA_DIR}\")\nprint(f\"Directory exists: {RSNA_DIR.exists()}\")\n\n# rsna-aneurysm ルートと絶対パスを定義\nRAW_DIR = RSNA_DIR / 'data' / 'raw'\nPROC_DIR = RSNA_DIR / 'data' / 'processed'\nRAW_DIR.mkdir(parents=True, exist_ok=True)\nPROC_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef run_module(mod: str, args: list[str], cwd: Path) -> None:\n    \"\"\"python -m 実行でstderrも表示して失敗理由を可視化\"\"\"\n    cmd = [sys.executable, '-m', mod, *args]\n    print('Running:', ' '.join(map(str, cmd)), '| cwd=', str(cwd))\n    try:\n        res = subprocess.run(cmd, cwd=str(cwd), check=True, capture_output=True, text=True)\n        if res.stdout:\n            print(res.stdout)\n        if res.stderr:\n            print(res.stderr)\n    except subprocess.CalledProcessError as e:\n        print('STDOUT:\\n', e.stdout)\n        print('STDERR:\\n', e.stderr)\n        raise\n\n# 1) Kaggle データダウンロード（存在すればスキップ）\n# 初期開発用: まずCSVファイルのみダウンロード（高速）\nif not (RAW_DIR / 'train.csv').exists():\n    print(\"Downloading essential CSV files first (fast download)...\")\n    run_module('scripts.download_data', [\n        '--competition', 'rsna-intracranial-aneurysm-detection',\n        '--output', str(RAW_DIR),\n        '--files-only'  # CSVファイルのみダウンロード\n    ], cwd=RSNA_DIR)\n    \n    # 画像データが必要な場合は以下のコメントを外す（時間がかかるため注意）\n    # print(\"Downloading large image datasets (this may take 10+ minutes)...\")\n    # run_module('scripts.download_data', [\n    #     '--competition', 'rsna-intracranial-aneurysm-detection', \n    #     '--output', str(RAW_DIR)\n    # ], cwd=RSNA_DIR)\nelse:\n    print('Skip download: train.csv found')\n\n# 画像データが存在しない場合の警告表示\nif not (RAW_DIR / 'train_images').exists() and not (RAW_DIR / 'train_images.zip').exists():\n    print(\"\\n\" + \"=\"*60)\n    print(\"⚠️  IMAGE DATA NOT DOWNLOADED\")\n    print(\"=\"*60)\n    print(\"Only CSV files were downloaded for fast development.\")\n    print(\"To download images, uncomment the lines above or run:\")\n    print(f\"python -m scripts.download_data --competition rsna-intracranial-aneurysm-detection --output {RAW_DIR}\")\n    print(\"=\"*60 + \"\\n\")\n\n# 2) DICOM メタデータ抽出（画像データがある場合のみ）\nif (RAW_DIR / 'train_images').exists() and not (PROC_DIR / 'train_metadata.csv').exists():\n    run_module('scripts.dicom_utils', [\n        'extract-metadata',\n        '--input', str(RAW_DIR / 'train_images'),\n        '--output', str(PROC_DIR / 'train_metadata.csv')\n    ], cwd=RSNA_DIR)\nelif not (RAW_DIR / 'train_images').exists():\n    print('Skip metadata extract: train_images directory not found')\nelse:\n    print('Skip metadata extract: train_metadata.csv found')\n\n# 3) DICOM → PNG 変換（画像データがある場合のみ）\nif (RAW_DIR / 'train_images').exists() and not any(PROC_DIR.glob('*.png')):\n    print('Converting DICOM to PNG images...')\n    run_module('scripts.dicom_utils', [\n        'convert-images',\n        '--input', str(RAW_DIR / 'train_images'),\n        '--output', str(PROC_DIR),\n        '--format', 'png', '--target-size', '512', '512', '--window-center', '40', '--window-width', '80'\n    ], cwd=RSNA_DIR)\nelif not (RAW_DIR / 'train_images').exists():\n    print('Skip convert: train_images directory not found')\nelse:\n    print('Skip convert: PNG already exists in processed dir')\n\n# 4) 学習用メタ生成（train.csvがあれば実行）\nif not (PROC_DIR / 'train.csv').exists():\n    if (PROC_DIR / 'train_metadata.csv').exists():\n        run_module('scripts.create_metadata', [\n            '--train-csv', str(RAW_DIR / 'train.csv'),\n            '--metadata', str(PROC_DIR / 'train_metadata.csv'),\n            '--output', str(PROC_DIR / 'train_processed.csv')\n        ], cwd=RSNA_DIR)\n        # cp の代わりにPythonでコピー\n        import shutil\n        shutil.copy2(PROC_DIR / 'train_processed.csv', PROC_DIR / 'train.csv')\n    else:\n        print('Creating simple training metadata from train.csv only...')\n        # メタデータなしで基本的な学習用データを作成\n        import pandas as pd\n        train_csv = pd.read_csv(RAW_DIR / 'train.csv')\n        # 必要最小限の前処理\n        train_csv.to_csv(PROC_DIR / 'train.csv', index=False)\nelse:\n    print('Skip create_metadata: processed/train.csv found')\n\nprint('Data preparation completed.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP/compile ユーティリティ\n",
    "import torch, os, random, numpy as np\n",
    "\n",
    "# 乱数固定と matmul 精度（2.x 推奨）\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# autocast の dtype を自動選択（bf16優先）\n",
    "from torch.amp import autocast\n",
    "\n",
    "def autocast_kwargs():\n",
    "    if torch.cuda.is_available():\n",
    "        use_bf16 = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
    "        return dict(enabled=True, device_type=\"cuda\",\n",
    "                    dtype=(torch.bfloat16 if use_bf16 else torch.float16))\n",
    "    return dict(enabled=False, device_type=\"cpu\")\n",
    "\n",
    "# torch.compile を環境変数でON/OFF\n",
    "\n",
    "def maybe_compile(model):\n",
    "    if os.getenv(\"USE_COMPILE\", \"0\") == \"1\" and hasattr(torch, \"compile\"):\n",
    "        try:\n",
    "            return torch.compile(model, mode=\"max-autotune\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] torch.compile disabled: {e}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# PyTorch関連\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# 画像処理\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pydicom\n",
    "\n",
    "# ML・評価指標\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定読み込み\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Experiment ID: {cfg['experiment']['id']}\")\n",
    "print(f\"Description: {cfg['experiment']['description']}\")\n",
    "print(f\"Model: {cfg['model']['architecture']}\")\n",
    "print(f\"Image Size: {cfg['data']['image_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード設定（再現性確保）\n",
    "def set_seed(seed):\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = cfg[\"environment\"][\"deterministic\"]\n",
    "    torch.backends.cudnn.benchmark = not cfg[\"environment\"][\"deterministic\"]\n",
    "\n",
    "\n",
    "set_seed(cfg[\"environment\"][\"seed\"])\n",
    "device = torch.device(cfg[\"environment\"][\"device\"])\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git SHA取得（バージョン管理）\n",
    "def get_git_sha():\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "git_sha = get_git_sha()\n",
    "print(f\"Git SHA: {git_sha}\")\n",
    "\n",
    "# Git SHA保存\n",
    "with open(\"git_sha.txt\", \"w\") as f:\n",
    "    f.write(git_sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "data_dir = Path(cfg[\"paths\"][\"data_dir\"])\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# データファイルの存在確認\n",
    "train_csv_path = data_dir / \"processed\" / \"train.csv\"\n",
    "if not train_csv_path.exists():\n",
    "    print(f\"Warning: {train_csv_path} not found. Creating sample data for testing...\")\n",
    "    # サンプルデータ作成（14ラベル: present + 13 loc_*）\n",
    "    n = 100\n",
    "    sample = {\n",
    "        \"image_id\": [f\"img_{i:04d}\" for i in range(n)],\n",
    "        \"PatientID\": [f\"patient_{i // 10:03d}\" for i in range(n)],\n",
    "        \"present\": np.random.choice([0, 1], n, p=[0.7, 0.3]),\n",
    "    }\n",
    "    for j in range(13):\n",
    "        sample[f\"loc_{j+1:02d}\"] = np.random.choice([0, 1], n, p=[0.9, 0.1])\n",
    "    sample_df = pd.DataFrame(sample)\n",
    "\n",
    "    # 保存ディレクトリ作成\n",
    "    (data_dir / \"processed\").mkdir(parents=True, exist_ok=True)\n",
    "    sample_df.to_csv(train_csv_path, index=False)\n",
    "    print(f\"Sample data saved to {train_csv_path}\")\n",
    "\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "if cfg[\"data\"][\"target_column\"] in train_df.columns:\n",
    "    print(\"Target distribution (single target):\")\n",
    "    print(train_df[cfg[\"data\"][\"target_column\"]].value_counts())\n",
    "elif \"present\" in train_df.columns:\n",
    "    print(\"Target distribution (present):\")\n",
    "    print(train_df[\"present\"].value_counts())\n",
    "else:\n",
    "    print(\"Target columns will be inferred later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CV: present で層化、study_id/PatientID でグループ ====\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import re\n",
    "\n",
    "# present 列を簡易検出（後段の詳細検出セルが未実行でも動くように）\n",
    "if 'present_col' not in locals():\n",
    "    pres = [c for c in train_df.columns if re.fullmatch(r\"(aneurysm_)?present\", c, flags=re.I)]\n",
    "    if not pres:\n",
    "        pres = [c for c in train_df.columns if c.lower() in {\"present\",\"aneurysm_present\",\"target\"}]\n",
    "    assert len(pres) >= 1, \"present 列が見つかりません（train.csv を確認してください）\"\n",
    "    present_col = pres[0]\n",
    "\n",
    "group_col = \"study_id\" if \"study_id\" in train_df.columns else (\"PatientID\" if \"PatientID\" in train_df.columns else cfg[\"cv\"][\"group_column\"])\n",
    "y_strat = train_df[present_col].astype(int)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=cfg[\"cv\"][\"n_folds\"], shuffle=True, random_state=cfg[\"cv\"][\"seed\"])\n",
    "train_df[\"fold\"] = -1\n",
    "for fold, (_, val_idx) in enumerate(sgkf.split(train_df, y_strat, groups=train_df[group_col])):\n",
    "    train_df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "# 進捗確認\n",
    "print(train_df.groupby(\"fold\")[present_col].value_counts().unstack(fill_value=0))\n",
    "train_df[[group_col, \"fold\", present_col]].to_csv(\"cv_folds.csv\", index=False)\n",
    "print(\"CV splits saved to cv_folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 14ラベル（present + 13 locations）を自動検出 ====\n",
    "import re\n",
    "\n",
    "def infer_label_columns(df):\n",
    "    pres = [c for c in df.columns if re.fullmatch(r\"(aneurysm_)?present\", c, flags=re.I)]\n",
    "    if not pres:\n",
    "        pres = [c for c in df.columns if c.lower() in {\"present\",\"aneurysm_present\",\"target\"}]\n",
    "    assert len(pres) >= 1, \"present 列が見つかりません（train.csv を確認してください）\"\n",
    "    present_col = pres[0]\n",
    "\n",
    "    binary_cols = []\n",
    "    for c in df.columns:\n",
    "        if c == present_col:\n",
    "            continue\n",
    "        s = df[c].dropna()\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            u = set(pd.Series(s).astype(float).round().astype(int).unique().tolist())\n",
    "        except Exception:\n",
    "            continue\n",
    "        if u.issubset({0,1}):\n",
    "            binary_cols.append(c)\n",
    "\n",
    "    prefer = [c for c in binary_cols if re.search(r\"(loc|location|artery|aca|mca|ica|pcom|acom|basilar|pca|sca|va|cavernous)\", c, flags=re.I)]\n",
    "    loc_cols = prefer if len(prefer) >= 13 else binary_cols\n",
    "    assert len(loc_cols) >= 13, f\"部位ラベルが不足（検出 {len(loc_cols)} 列）。train.csv の列名を確認してください。\"\n",
    "    loc_cols = loc_cols[:13]\n",
    "\n",
    "    return present_col, loc_cols\n",
    "\n",
    "present_col, loc_cols = infer_label_columns(train_df)\n",
    "label_cols = [present_col] + loc_cols\n",
    "num_classes = len(label_cols)\n",
    "print(\"Detected label columns:\", label_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張設定\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.Rotate(limit=10, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "                A.CLAHE(p=0.3),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset定義\n",
    "class AneurysmDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, mode=\"train\", label_cols=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.label_cols = label_cols or []\n",
    "\n",
    "        if not self.image_dir.exists():\n",
    "            print(f\"Warning: Image directory {self.image_dir} does not exist. Creating dummy images for testing...\")\n",
    "            self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._create_dummy_images()\n",
    "\n",
    "    def _create_dummy_images(self):\n",
    "        for idx, row in self.df.iterrows():\n",
    "            image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "            if not image_path.exists():\n",
    "                dummy_image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "                cv2.imwrite(str(image_path), dummy_image)\n",
    "        print(f\"Created {len(self.df)} dummy images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            label = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class AneurysmModel(nn.Module):\n",
    "    def __init__(self, model_name=\"resnet50\", num_classes=14, pretrained=True):\n",
    "        super().__init__()\n",
    "        if model_name == \"resnet50\":\n",
    "            self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(\"Only resnet50 is implemented in this baseline.\")\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(in_features, cfg[\"model\"][\"hidden_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(cfg[\"model\"][\"hidden_dim\"], num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.classifier(feat)\n",
    "\n",
    "\n",
    "# モデル初期化テスト\n",
    "model = AneurysmModel(\n",
    "    model_name=cfg[\"model\"][\"architecture\"], num_classes=num_classes, pretrained=cfg[\"model\"][\"pretrained\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "model = maybe_compile(model)\n",
    "print(f\"Model created: {cfg['model']['architecture']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B初期化\n",
    "run = wandb.init(\n",
    "    project=cfg[\"logging\"][\"wandb\"][\"project\"],\n",
    "    name=cfg[\"logging\"][\"wandb\"][\"name\"],\n",
    "    config=cfg,\n",
    "    tags=cfg[\"logging\"][\"wandb\"][\"tags\"],\n",
    "    notes=cfg[\"logging\"][\"wandb\"][\"notes\"],\n",
    ")\n",
    "\n",
    "# W&B URL保存\n",
    "wandb_url = f\"https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\"\n",
    "with open(\"wandb_run.txt\", \"w\") as f:\n",
    "    f.write(f\"URL: {wandb_url}\\n\")\n",
    "    f.write(f\"Run ID: {wandb.run.id}\\n\")\n",
    "\n",
    "print(f\"W&B run: {wandb_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・評価関数（14ラベル対応, weighted AUC）\n",
    "def train_epoch(model, dataloader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(**autocast_kwargs()):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        if cfg[\"environment\"][\"mixed_precision\"]:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds_all, targs_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with autocast(**autocast_kwargs()):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds_all.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            targs_all.append(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    preds = np.concatenate(preds_all, axis=0)\n",
    "    targs = np.concatenate(targs_all, axis=0)\n",
    "    aucs = []\n",
    "    for j in range(preds.shape[1]):\n",
    "        y_true = targs[:, j]\n",
    "        y_pred = preds[:, j]\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            aucs.append(np.nan)\n",
    "        else:\n",
    "            aucs.append(roc_auc_score(y_true, y_pred))\n",
    "    aucs = np.array(aucs, dtype=float)\n",
    "    weights = np.array([13] + [1]*13, dtype=float)[: preds.shape[1]]\n",
    "    valid = ~np.isnan(aucs)\n",
    "    weighted_auc = np.average(aucs[valid], weights=weights[valid]) if valid.any() else np.nan\n",
    "    return avg_loss, float(weighted_auc), preds, targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation（14ラベル対応）\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros((len(train_df), num_classes), dtype=np.float32)\n",
    "oof_targets = train_df[label_cols].values.astype(\"float32\")\n",
    "\n",
    "# モデル保存ディレクトリ作成\n",
    "Path(\"model\").mkdir(exist_ok=True)\n",
    "\n",
    "for fold in range(cfg[\"cv\"][\"n_folds\"]):\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"FOLD {fold + 1}/{cfg['cv']['n_folds']}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # データ分割\n",
    "    train_fold_df = train_df[train_df[\"fold\"] != fold]\n",
    "    valid_fold_df = train_df[train_df[\"fold\"] == fold]\n",
    "\n",
    "    print(f\"Train: {len(train_fold_df)}, Valid: {len(valid_fold_df)}\")\n",
    "\n",
    "    # Dataset・DataLoader作成（label_cols を渡す）\n",
    "    train_dataset = AneurysmDataset(train_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"train\"), label_cols=label_cols)\n",
    "    valid_dataset = AneurysmDataset(valid_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"valid\"), label_cols=label_cols)\n",
    "\n",
    "    # DataLoader: Colab向けに調整\n",
    "    num_workers = min(4, os.cpu_count() or 2)\n",
    "    use_pin = torch.cuda.is_available()\n",
    "    use_persist = num_workers > 0\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_pin,\n",
    "        persistent_workers=use_persist,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_pin,\n",
    "        persistent_workers=use_persist,\n",
    "    )\n",
    "\n",
    "    # モデル・最適化設定\n",
    "    model = AneurysmModel(\n",
    "        model_name=cfg[\"model\"][\"architecture\"], num_classes=num_classes, pretrained=cfg[\"model\"][\"pretrained\"]\n",
    "    ).to(device)\n",
    "    model = maybe_compile(model)\n",
    "\n",
    "    optimizer_type = cfg[\"train\"][\"optimizer\"][\"type\"]\n",
    "    if optimizer_type == \"AdamW\":\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=cfg[\"environment\"][\"mixed_precision\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=cfg[\"train\"][\"scheduler\"][\"T_0\"], eta_min=cfg[\"train\"][\"scheduler\"][\"eta_min\"]\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    best_auc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg[\"train\"][\"epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
    "        valid_loss, valid_auc, valid_preds, valid_targets = validate_epoch(model, valid_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{cfg['train']['epochs']}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid AUC: {valid_auc:.4f}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"fold_{fold}_train_loss\": train_loss,\n",
    "                f\"fold_{fold}_valid_loss\": valid_loss,\n",
    "                f\"fold_{fold}_valid_auc\": valid_auc,\n",
    "                \"epoch\": epoch,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"model/fold_{fold}_best.pth\")\n",
    "            best_preds = valid_preds.copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= cfg[\"train\"][\"early_stopping\"][\"patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # OOF予測保存（行列）\n",
    "    valid_indices = valid_fold_df.index\n",
    "    oof_predictions[valid_indices] = best_preds\n",
    "\n",
    "    fold_scores.append(best_auc)\n",
    "    print(f\"Fold {fold + 1} Best AUC: {best_auc:.4f}\")\n",
    "\n",
    "    # メモリクリア\n",
    "    del model, optimizer, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# CV結果集計\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "\n",
    "# OOF weighted AUC（列ごとAUCの重み付き平均）\n",
    "aucs = []\n",
    "for j in range(oof_predictions.shape[1]):\n",
    "    y_true = oof_targets[:, j]\n",
    "    y_pred = oof_predictions[:, j]\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        aucs.append(np.nan)\n",
    "    else:\n",
    "        aucs.append(roc_auc_score(y_true, y_pred))\n",
    "aucs = np.array(aucs, dtype=float)\n",
    "weights = np.array([13] + [1]*13, dtype=float)[: oof_predictions.shape[1]]\n",
    "valid = ~np.isnan(aucs)\n",
    "oof_auc = np.average(aucs[valid], weights=weights[valid]) if valid.any() else np.nan\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"CV RESULTS\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Fold Scores: {fold_scores}\")\n",
    "print(f\"CV Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"OOF AUC (weighted): {oof_auc:.4f}\")\n",
    "\n",
    "wandb.log({\"cv_mean_auc\": cv_mean, \"cv_std_auc\": cv_std, \"oof_auc\": float(oof_auc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF結果保存（14ラベル、列名付き）\n",
    "oof_df = pd.DataFrame({\"index\": train_df.index, \"fold\": train_df[\"fold\"]})\n",
    "for i, c in enumerate(label_cols):\n",
    "    oof_df[f\"y_true_{c}\"] = oof_targets[:, i]\n",
    "    oof_df[f\"y_pred_{c}\"] = oof_predictions[:, i]\n",
    "oof_df.to_csv(\"oof_predictions.csv\", index=False)\n",
    "\n",
    "# メトリクス保存\n",
    "metrics = {\n",
    "    \"experiment_id\": cfg[\"experiment\"][\"id\"],\n",
    "    \"cv_mean_auc\": float(cv_mean),\n",
    "    \"cv_std_auc\": float(cv_std),\n",
    "    \"oof_auc\": float(oof_auc),\n",
    "    \"fold_scores\": [float(x) for x in fold_scores],\n",
    "    \"git_sha\": git_sha,\n",
    "    \"wandb_url\": wandb_url,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(\"- oof_predictions.csv\")\n",
    "print(\"- metrics.json\")\n",
    "print(f\"- model/fold_*_best.pth ({cfg['cv']['n_folds']} files)\")\n",
    "print(\"- wandb_run.txt\")\n",
    "print(\"- git_sha.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive に成果物バックアップ\n",
    "backup_dir = f\"/content/drive/MyDrive/rsna-aneurysm/{cfg['experiment']['id']}\"\n",
    "!mkdir -p \"{backup_dir}\"\n",
    "!cp oof_predictions.csv \"{backup_dir}/\"\n",
    "!cp metrics.json \"{backup_dir}/\"\n",
    "!cp -r model \"{backup_dir}/\"\n",
    "!cp wandb_run.txt \"{backup_dir}/\"\n",
    "!cp git_sha.txt \"{backup_dir}/\"\n",
    "\n",
    "print(f\"Results backed up to: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B終了\n",
    "wandb.finish()\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}