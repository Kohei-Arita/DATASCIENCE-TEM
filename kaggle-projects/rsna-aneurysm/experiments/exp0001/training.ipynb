{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Aneurysm Detection Training - exp0001\n",
    "\n",
    "**実験概要**: ResNet50 ベースラインモデル\n",
    "\n",
    "**実行環境**: \n",
    "- Google Colab (GPU)\n",
    "- PyTorch with Mixed Precision\n",
    "- W&B Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# GPU確認\n",
    "!nvidia-smi\n",
    "\n",
    "# リポジトリクローン（初回のみ）\n",
    "repo_name = \"DATASCIENCE-TEM\"\n",
    "repo_url = os.environ.get(\"REPO_URL\", \"https://github.com/YOUR_USERNAME/DATASCIENCE-TEM.git\")\n",
    "\n",
    "if not os.path.exists(f\"/content/{repo_name}\"):\n",
    "    print(f\"Cloning {repo_url}...\")\n",
    "    !git clone {repo_url}\n",
    "    %cd /content/{repo_name}\n",
    "else:\n",
    "    print(f\"Repository {repo_name} already exists, updating...\")\n",
    "    %cd /content/{repo_name}\n",
    "    !git pull origin main || true\n",
    "\n",
    "# プロジェクトルートをPythonパスに追加（重要：scripts.moduleを見つけるため）\n",
    "project_root = f\"/content/{repo_name}\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# 実験ディレクトリへ移動（常に明示）\n",
    "%cd /content/DATASCIENCE-TEM/kaggle-projects/rsna-aneurysm/experiments/exp0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係インストール（Colab向け）\n",
    "# - PyTorch は cu121 ホイールを明示（Colab の CUDA12 系に整合）\n",
    "# - torch 以外の依存のみ個別に追加\n",
    "!pip -q install --upgrade pip\n",
    "\n",
    "# PyTorch 2.4.1 + cu121（torchvision/torchaudio も整合）\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n",
    "\n",
    "# プロジェクト依存（torch 以外）\n",
    "!pip -q install timm==1.0.16 albumentations==2.0.8 opencv-python-headless==4.10.0.84 \\\n",
    "  nibabel>=5.2.1 pydicom>=2.4.4 monai>=1.4.0 lightning==2.4.0 accelerate==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API認証設定\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "# W&B オフラインフォールバック\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "    print(\"[info] WANDB_API_KEY not found -> running in offline mode\")\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
    "\n",
    "# Google Drive マウント（成果物保存用）\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle CLI 用の認証ファイル作成（環境変数から生成）\n",
    "import os, json, pathlib, subprocess\n",
    "kdir = pathlib.Path.home().joinpath('.kaggle')\n",
    "kdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(kdir.joinpath('kaggle.json'), 'w') as f:\n",
    "    f.write(json.dumps({'username': os.environ.get('KAGGLE_USERNAME', ''), 'key': os.environ.get('KAGGLE_KEY', '')}))\n",
    "kjson = kdir.joinpath('kaggle.json')\n",
    "try:\n",
    "    subprocess.run(['chmod', '600', str(kjson)], check=False)\n",
    "except Exception as e:\n",
    "    print('chmod failed:', e)\n",
    "print('Kaggle credentials prepared at ~/.kaggle/kaggle.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ準備（Kaggleダウンロード→メタ抽出→PNG変換→学習用メタ生成）\n",
    "from pathlib import Path\n",
    "import os, sys, subprocess\n",
    "\n",
    "# 作業パス確認（このノートは exp0001 直下で動かす想定）\n",
    "print('CWD:', os.getcwd())\n",
    "\n",
    "# RSNA_DIR を確実に特定（Colab での二重ネストを回避）\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "def detect_rsna_dir() -> Path:\n",
    "    # Colabではリポジトリを /content/DATASCIENCE-TEM にクローンしている前提\n",
    "    root_candidates = [\n",
    "        Path(\"/content/DATASCIENCE-TEM\"),\n",
    "        Path(\"/content\") / os.environ.get(\"REPO_NAME\", \"DATASCIENCE-TEM\"),\n",
    "    ]\n",
    "    root_candidates = [p for p in root_candidates if p.exists()]\n",
    "    if root_candidates:\n",
    "        project_root = root_candidates[0]\n",
    "    else:\n",
    "        # 最後の保険：git ルートを探す\n",
    "        try:\n",
    "            project_root = Path(subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).decode().strip())\n",
    "        except Exception:\n",
    "            project_root = Path(\"/content/DATASCIENCE-TEM\")\n",
    "    print(f\"[resolved] project_root={project_root}\")\n",
    "    return project_root / \"kaggle-projects\" / \"rsna-aneurysm\"\n",
    "\n",
    "RSNA_DIR = detect_rsna_dir()\n",
    "print(f\"RSNA project directory: {RSNA_DIR}\")\n",
    "print(f\"Directory exists: {RSNA_DIR.exists()}\")\n",
    "\n",
    "# rsna-aneurysm ルートと絶対パスを定義\n",
    "RAW_DIR = RSNA_DIR / 'data' / 'raw'\n",
    "PROC_DIR = RSNA_DIR / 'data' / 'processed'\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_module(mod: str, args: list[str], cwd: Path) -> None:\n",
    "    \"\"\"python -m 実行でstderrも表示して失敗理由を可視化\"\"\"\n",
    "    cmd = [sys.executable, '-m', mod, *args]\n",
    "    print('Running:', ' '.join(map(str, cmd)), '| cwd=', str(cwd))\n",
    "    try:\n",
    "        res = subprocess.run(cmd, cwd=str(cwd), check=True, capture_output=True, text=True)\n",
    "        if res.stdout:\n",
    "            print(res.stdout)\n",
    "        if res.stderr:\n",
    "            print(res.stderr)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('STDOUT:\\n', e.stdout)\n",
    "        print('STDERR:\\n', e.stderr)\n",
    "        raise\n",
    "\n",
    "# Module 存在チェック\n",
    "\n",
    "def have_module(mod_name: str) -> bool:\n",
    "    import importlib.util\n",
    "    return importlib.util.find_spec(mod_name) is not None\n",
    "\n",
    "# 1) Kaggle データダウンロード（存在すればスキップ）\n",
    "# 初期開発用: まずCSVファイルのみダウンロード（高速）\n",
    "if not (RAW_DIR / 'train.csv').exists():\n",
    "    print(\"Downloading essential CSV files first (fast download)...\")\n",
    "    use_scripts = have_module('kaggle-projects.rsna-aneurysm.scripts.download_data') or have_module('scripts.download_data')\n",
    "    if use_scripts:\n",
    "        # プロジェクトルートから実行\n",
    "        project_root = RSNA_DIR.parent.parent\n",
    "        mod_name = 'kaggle-projects.rsna-aneurysm.scripts.download_data' if have_module('kaggle-projects.rsna-aneurysm.scripts.download_data') else 'scripts.download_data'\n",
    "        run_module(mod_name, [\n",
    "            '--competition', 'rsna-intracranial-aneurysm-detection',\n",
    "            '--output', str(RAW_DIR),\n",
    "            '--files-only'\n",
    "        ], cwd=project_root)\n",
    "    else:\n",
    "        print(\"[warn] scripts.download_data not found -> using Kaggle CLI\")\n",
    "        # Kaggle CLI で一括 zip ダウンロード→解凍（CSV取得）\n",
    "        import shlex\n",
    "        zip_path = RAW_DIR / 'rsna-intracranial-aneurysm-detection.zip'\n",
    "        cmd = f\"kaggle competitions download -c rsna-intracranial-aneurysm-detection -p {shlex.quote(str(RAW_DIR))} --force\"\n",
    "        print('Running:', cmd)\n",
    "        os.system(cmd)\n",
    "        if zip_path.exists():\n",
    "            unzip_cmd = f\"unzip -o {shlex.quote(str(zip_path))} -d {shlex.quote(str(RAW_DIR))}\"\n",
    "            print('Running:', unzip_cmd)\n",
    "            os.system(unzip_cmd)\n",
    "        else:\n",
    "            print('[error] expected zip not found:', zip_path)\n",
    "else:\n",
    "    print('Skip download: train.csv found')\n",
    "\n",
    "# 画像データが存在しない場合の警告表示\n",
    "if not (RAW_DIR / 'train_images').exists() and not (RAW_DIR / 'train_images.zip').exists():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⚠️  IMAGE DATA NOT DOWNLOADED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Only CSV files were downloaded for fast development.\")\n",
    "    print(\"To download images, uncomment the lines above or run:\")\n",
    "    print(f\"python -m kaggle-projects.rsna-aneurysm.scripts.download_data --competition rsna-intracranial-aneurysm-detection --output {RAW_DIR}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2) DICOM メタデータ抽出（画像データがある場合のみ）\n",
    "if (RAW_DIR / 'train_images').exists() and not (PROC_DIR / 'train_metadata.csv').exists():\n",
    "    project_root = RSNA_DIR.parent.parent\n",
    "    run_module('kaggle-projects.rsna-aneurysm.scripts.dicom_utils', [\n",
    "        'extract-metadata',\n",
    "        '--input', str(RAW_DIR / 'train_images'),\n",
    "        '--output', str(PROC_DIR / 'train_metadata.csv')\n",
    "    ], cwd=project_root)\n",
    "elif not (RAW_DIR / 'train_images').exists():\n",
    "    print('Skip metadata extract: train_images directory not found')\n",
    "else:\n",
    "    print('Skip metadata extract: train_metadata.csv found')\n",
    "\n",
    "# 3) DICOM → PNG 変換（画像データがある場合のみ）\n",
    "if (RAW_DIR / 'train_images').exists() and not any(PROC_DIR.glob('*.png')):\n",
    "    print('Converting DICOM to PNG images...')\n",
    "    project_root = RSNA_DIR.parent.parent\n",
    "    if have_module('kaggle-projects.rsna-aneurysm.scripts.dicom_utils') or have_module('scripts.dicom_utils'):\n",
    "        mod_name = 'kaggle-projects.rsna-aneurysm.scripts.dicom_utils' if have_module('kaggle-projects.rsna-aneurysm.scripts.dicom_utils') else 'scripts.dicom_utils'\n",
    "        run_module(mod_name, [\n",
    "            'convert-images',\n",
    "            '--input', str(RAW_DIR / 'train_images'),\n",
    "            '--output', str(PROC_DIR),\n",
    "            '--format', 'png', '--target-size', '512', '512'\n",
    "        ], cwd=project_root)\n",
    "    else:\n",
    "        print(\"[warn] scripts.dicom_utils not found -> using minimal converter\")\n",
    "        from glob import glob\n",
    "        import pydicom, numpy as np, cv2, os\n",
    "        paths = glob(str(RAW_DIR / 'train_images' / '**' / '*.dcm'), recursive=True)\n",
    "        for dp in paths:\n",
    "            try:\n",
    "                ds = pydicom.dcmread(dp, force=True)\n",
    "                arr = ds.pixel_array.astype(np.float32)\n",
    "                slope = float(getattr(ds, 'RescaleSlope', 1.0))\n",
    "                inter = float(getattr(ds, 'RescaleIntercept', 0.0))\n",
    "                arr = arr * slope + inter\n",
    "                img = arr\n",
    "                img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "                img = (img * 255.0).clip(0, 255).astype(np.uint8)\n",
    "                img = cv2.resize(img, (512, 512))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                out_name = os.path.splitext(os.path.basename(dp))[0] + '.png'\n",
    "                cv2.imwrite(str(PROC_DIR / out_name), img)\n",
    "            except Exception as e:\n",
    "                print('[skip] convert failed:', dp, e)\n",
    "elif not (RAW_DIR / 'train_images').exists():\n",
    "    print('Skip convert: train_images directory not found')\n",
    "else:\n",
    "    print('Skip convert: PNG already exists in processed dir')\n",
    "\n",
    "# 4) 学習用メタ生成（train.csvがあれば実行）\n",
    "if not (PROC_DIR / 'train.csv').exists():\n",
    "    if (PROC_DIR / 'train_metadata.csv').exists():\n",
    "        project_root = RSNA_DIR.parent.parent\n",
    "        if have_module('kaggle-projects.rsna-aneurysm.scripts.create_metadata') or have_module('scripts.create_metadata'):\n",
    "            mod_name = 'kaggle-projects.rsna-aneurysm.scripts.create_metadata' if have_module('kaggle-projects.rsna-aneurysm.scripts.create_metadata') else 'scripts.create_metadata'\n",
    "            run_module(mod_name, [\n",
    "                '--train-csv', str(RAW_DIR / 'train.csv'),\n",
    "                '--metadata', str(PROC_DIR / 'train_metadata.csv'),\n",
    "                '--output', str(PROC_DIR / 'train_processed.csv')\n",
    "            ], cwd=project_root)\n",
    "            import shutil\n",
    "            shutil.copy2(PROC_DIR / 'train_processed.csv', PROC_DIR / 'train.csv')\n",
    "        else:\n",
    "            print('[warn] scripts.create_metadata not found -> falling back to simple copy')\n",
    "            import pandas as pd\n",
    "            train_csv = pd.read_csv(RAW_DIR / 'train.csv')\n",
    "            train_csv.to_csv(PROC_DIR / 'train.csv', index=False)\n",
    "    else:\n",
    "        print('Creating simple training metadata from train.csv only...')\n",
    "        import pandas as pd\n",
    "        train_csv = pd.read_csv(RAW_DIR / 'train.csv')\n",
    "        train_csv.to_csv(PROC_DIR / 'train.csv', index=False)\n",
    "else:\n",
    "    print('Skip create_metadata: processed/train.csv found')\n",
    "\n",
    "print('Data preparation completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP/compile ユーティリティ\n",
    "import torch, os, random, numpy as np\n",
    "\n",
    "# 乱数固定と matmul 精度（2.x 推奨）\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# autocast の dtype を自動選択（bf16優先）\n",
    "from torch.amp import autocast\n",
    "\n",
    "def autocast_kwargs():\n",
    "    if torch.cuda.is_available():\n",
    "        use_bf16 = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
    "        return dict(enabled=True, device_type=\"cuda\",\n",
    "                    dtype=(torch.bfloat16 if use_bf16 else torch.float16))\n",
    "    return dict(enabled=False, device_type=\"cpu\")\n",
    "\n",
    "# torch.compile を環境変数でON/OFF\n",
    "\n",
    "def maybe_compile(model):\n",
    "    if os.getenv(\"USE_COMPILE\", \"0\") == \"1\" and hasattr(torch, \"compile\"):\n",
    "        try:\n",
    "            return torch.compile(model, mode=\"max-autotune\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] torch.compile disabled: {e}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# PyTorch関連\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# 画像処理\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pydicom\n",
    "\n",
    "# ML・評価指標\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定読み込み\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Experiment ID: {cfg['experiment']['id']}\")\n",
    "print(f\"Description: {cfg['experiment']['description']}\")\n",
    "print(f\"Model: {cfg['model']['architecture']}\")\n",
    "print(f\"Image Size: {cfg['data']['image_size']}\")\n",
    "\n",
    "# Colab 実行時のパスを RSNA_DIR に強制同期\n",
    "try:\n",
    "    from pathlib import Path as _Path\n",
    "    _rsna_dir = RSNA_DIR if 'RSNA_DIR' in globals() else _Path.cwd().parents[2] / 'kaggle-projects' / 'rsna-aneurysm'\n",
    "    cfg.setdefault('paths', {})\n",
    "    cfg['paths']['data_dir'] = str(_rsna_dir / 'data')\n",
    "    cfg['paths']['raw_data'] = str(_rsna_dir / 'data' / 'raw')\n",
    "    cfg['paths']['processed_data'] = str(_rsna_dir / 'data' / 'processed')\n",
    "    print(\"[paths] override ->\", cfg['paths'])\n",
    "except Exception as _e:\n",
    "    print('[warn] failed to override cfg paths:', _e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATA SANITY CHECK for RSNA IAD ====\n",
    "from pathlib import Path as _P\n",
    "import pandas as _pd\n",
    "import numpy as _np\n",
    "import re as _re\n",
    "\n",
    "_data_dir = _P(cfg[\"paths\"][\"data_dir\"])\n",
    "_train_csv_path = _data_dir / \"processed\" / \"train.csv\"\n",
    "_raw_train_csv = _data_dir / \"raw\" / \"train.csv\"\n",
    "_localizers_csv = _data_dir / \"raw\" / \"train_localizers.csv\"\n",
    "\n",
    "def _read_with_fallback(p1: _P, p2: _P) -> _pd.DataFrame:\n",
    "    if p1.exists():\n",
    "        return _pd.read_csv(p1)\n",
    "    if p2.exists():\n",
    "        return _pd.read_csv(p2)\n",
    "    raise FileNotFoundError(f\"train.csv not found at {p1} or {p2}\")\n",
    "\n",
    "_df = _read_with_fallback(_train_csv_path, _raw_train_csv)\n",
    "print(\"[sanity] train shape:\", _df.shape)\n",
    "print(\"[sanity] columns:\", list(_df.columns)[:40], \"...\")\n",
    "\n",
    "_pres = [c for c in _df.columns if _re.fullmatch(r\"(aneurysm_)?present\", c, flags=_re.I)]\n",
    "if not _pres:\n",
    "    _pres = [c for c in _df.columns if c.lower() in {\"present\",\"aneurysm_present\",\"target\"}]\n",
    "assert _pres, \"present 列が見つかりません（train.csv を確認）\"\n",
    "_present_col = _pres[0]\n",
    "\n",
    "_binary_cols = []\n",
    "for c in _df.columns:\n",
    "    if c == _present_col:\n",
    "        continue\n",
    "    s = _df[c].dropna()\n",
    "    if len(s) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        u = set(_pd.Series(s).astype(float).round().astype(int).unique().tolist())\n",
    "    except Exception:\n",
    "        continue\n",
    "    if u.issubset({0,1}):\n",
    "        _binary_cols.append(c)\n",
    "\n",
    "_kw = r\"(loc|location|artery|aca|mca|ica|pcom|acom|basilar|pca|sca|va|carotid|communicating|cavernous|ophthalmic|internal|middle|posterior|anterior)\"\n",
    "_prefer = [c for c in _binary_cols if _re.search(_kw, c, flags=_re.I)]\n",
    "\n",
    "_loc_cols = _prefer if len(_prefer) >= 13 else _binary_cols\n",
    "assert len(_loc_cols) >= 13, f\"部位ラベルが不足：検出 {len(_loc_cols)} 列。train.csv の列名を確認してください。\"\n",
    "_loc_cols = _loc_cols[:13]\n",
    "\n",
    "_label_cols = [_present_col] + _loc_cols\n",
    "print(\"[sanity] detected label columns (14):\", _label_cols)\n",
    "\n",
    "print(\"[sanity] present value counts:\\n\", _df[_present_col].value_counts(dropna=False))\n",
    "_null_rates = _df[_label_cols].isna().mean().round(4)\n",
    "print(\"[sanity] null rates (label cols):\\n\", _null_rates)\n",
    "\n",
    "_candidate_groups = [\"PatientID\", \"StudyInstanceUID\", \"SeriesInstanceUID\", \"study_id\"]\n",
    "_group_col = next((c for c in _candidate_groups if c in _df.columns), None)\n",
    "print(f\"[sanity] group column: {_group_col or '(not found)'}\")\n",
    "\n",
    "_has_localizers = _localizers_csv.exists()\n",
    "print(\"[sanity] train_localizers.csv:\", \"FOUND\" if _has_localizers else \"NOT FOUND\")\n",
    "\n",
    "print(\"[note] 公式評価は 14 列の AUC、present に重み13, 他1 の加重平均です。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード設定（再現性確保）\n",
    "def set_seed(seed):\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = cfg[\"environment\"][\"deterministic\"]\n",
    "    torch.backends.cudnn.benchmark = not cfg[\"environment\"][\"deterministic\"]\n",
    "\n",
    "\n",
    "set_seed(cfg[\"environment\"][\"seed\"])\n",
    "device = torch.device(cfg[\"environment\"][\"device\"])\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git SHA取得（バージョン管理）\n",
    "def get_git_sha():\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "git_sha = get_git_sha()\n",
    "print(f\"Git SHA: {git_sha}\")\n",
    "\n",
    "# Git SHA保存\n",
    "with open(\"git_sha.txt\", \"w\") as f:\n",
    "    f.write(git_sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "data_dir = Path(cfg[\"paths\"][\"data_dir\"])\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# データファイルの存在確認\n",
    "train_csv_path = data_dir / \"processed\" / \"train.csv\"\n",
    "if not train_csv_path.exists():\n",
    "    print(f\"Warning: {train_csv_path} not found. Creating sample data for testing...\")\n",
    "    # サンプルデータ作成（14ラベル: present + 13 loc_*）\n",
    "    n = 100\n",
    "    sample = {\n",
    "        \"image_id\": [f\"img_{i:04d}\" for i in range(n)],\n",
    "        \"PatientID\": [f\"patient_{i // 10:03d}\" for i in range(n)],\n",
    "        \"present\": np.random.choice([0, 1], n, p=[0.7, 0.3]),\n",
    "    }\n",
    "    for j in range(13):\n",
    "        sample[f\"loc_{j+1:02d}\"] = np.random.choice([0, 1], n, p=[0.9, 0.1])\n",
    "    sample_df = pd.DataFrame(sample)\n",
    "\n",
    "    # 保存ディレクトリ作成\n",
    "    (data_dir / \"processed\").mkdir(parents=True, exist_ok=True)\n",
    "    sample_df.to_csv(train_csv_path, index=False)\n",
    "    print(f\"Sample data saved to {train_csv_path}\")\n",
    "\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "if cfg[\"data\"][\"target_column\"] in train_df.columns:\n",
    "    print(\"Target distribution (single target):\")\n",
    "    print(train_df[cfg[\"data\"][\"target_column\"]].value_counts())\n",
    "elif \"present\" in train_df.columns:\n",
    "    print(\"Target distribution (present):\")\n",
    "    print(train_df[\"present\"].value_counts())\n",
    "else:\n",
    "    print(\"Target columns will be inferred later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CV: present で層化、study_id/PatientID でグループ ====\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import re\n",
    "\n",
    "# present 列を簡易検出（後段の詳細検出セルが未実行でも動くように）\n",
    "if 'present_col' not in locals():\n",
    "    pres = [c for c in train_df.columns if re.fullmatch(r\"(aneurysm_)?present\", c, flags=re.I)]\n",
    "    if not pres:\n",
    "        pres = [c for c in train_df.columns if c.lower() in {\"present\",\"aneurysm_present\",\"target\"}]\n",
    "    assert len(pres) >= 1, \"present 列が見つかりません（train.csv を確認してください）\"\n",
    "    present_col = pres[0]\n",
    "\n",
    "    # 後方互換なグループ列検出\n",
    "    candidate_groups = [\"PatientID\", \"StudyInstanceUID\", \"study_id\", \"SeriesInstanceUID\"]\n",
    "    group_col = next((c for c in candidate_groups if c in train_df.columns), None)\n",
    "    if group_col is None:\n",
    "        group_col = cfg[\"cv\"][\"group_column\"]\n",
    "    print(f\"[info] group column = {group_col}\")\n",
    "\n",
    "    y_strat = train_df[present_col].astype(int)\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=cfg[\"cv\"][\"n_folds\"], shuffle=True, random_state=cfg[\"cv\"][\"seed\"])\n",
    "    train_df[\"fold\"] = -1\n",
    "    for fold, (_, val_idx) in enumerate(sgkf.split(train_df, y_strat, groups=train_df[group_col])):\n",
    "        train_df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "    # 進捗確認\n",
    "    print(train_df.groupby(\"fold\")[present_col].value_counts().unstack(fill_value=0))\n",
    "    train_df[[group_col, \"fold\", present_col]].to_csv(\"cv_folds.csv\", index=False)\n",
    "    print(\"CV splits saved to cv_folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 14ラベル（present + 13 locations）を自動検出 ====\n",
    "import re\n",
    "\n",
    "def infer_label_columns(df):\n",
    "    pres = [c for c in df.columns if re.fullmatch(r\"(aneurysm_)?present\", c, flags=re.I)]\n",
    "    if not pres:\n",
    "        pres = [c for c in df.columns if c.lower() in {\"present\",\"aneurysm_present\",\"target\"}]\n",
    "    assert len(pres) >= 1, \"present 列が見つかりません（train.csv を確認してください）\"\n",
    "    present_col = pres[0]\n",
    "\n",
    "    binary_cols = []\n",
    "    for c in df.columns:\n",
    "        if c == present_col:\n",
    "            continue\n",
    "        s = df[c].dropna()\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            u = set(pd.Series(s).astype(float).round().astype(int).unique().tolist())\n",
    "        except Exception:\n",
    "            continue\n",
    "        if u.issubset({0,1}):\n",
    "            binary_cols.append(c)\n",
    "\n",
    "    prefer = [c for c in binary_cols if re.search(r\"(loc|location|artery|aca|mca|ica|pcom|acom|basilar|pca|sca|va|cavernous)\", c, flags=re.I)]\n",
    "    loc_cols = prefer if len(prefer) >= 13 else binary_cols\n",
    "    assert len(loc_cols) >= 13, f\"部位ラベルが不足（検出 {len(loc_cols)} 列）。train.csv の列名を確認してください。\"\n",
    "    loc_cols = loc_cols[:13]\n",
    "\n",
    "    return present_col, loc_cols\n",
    "\n",
    "present_col, loc_cols = infer_label_columns(train_df)\n",
    "label_cols = [present_col] + loc_cols\n",
    "num_classes = len(label_cols)\n",
    "print(\"Detected label columns:\", label_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張設定\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.Rotate(limit=10, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "                A.CLAHE(p=0.3),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "                A.Normalize(\n",
    "                    mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset定義\n",
    "class AneurysmDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, mode=\"train\", label_cols=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.label_cols = label_cols or []\n",
    "\n",
    "        if not self.image_dir.exists():\n",
    "            print(f\"Warning: Image directory {self.image_dir} does not exist. Creating dummy images for testing...\")\n",
    "            self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._create_dummy_images()\n",
    "\n",
    "    def _create_dummy_images(self):\n",
    "        for idx, row in self.df.iterrows():\n",
    "            image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "            if not image_path.exists():\n",
    "                dummy_image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "                cv2.imwrite(str(image_path), dummy_image)\n",
    "        print(f\"Created {len(self.df)} dummy images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            image = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            label = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class AneurysmModel(nn.Module):\n",
    "    def __init__(self, model_name=\"resnet50\", num_classes=14, pretrained=True):\n",
    "        super().__init__()\n",
    "        if model_name == \"resnet50\":\n",
    "            self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(\"Only resnet50 is implemented in this baseline.\")\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(in_features, cfg[\"model\"][\"hidden_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(cfg[\"model\"][\"hidden_dim\"], num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.classifier(feat)\n",
    "\n",
    "\n",
    "# モデル初期化テスト\n",
    "model = AneurysmModel(\n",
    "    model_name=cfg[\"model\"][\"architecture\"], num_classes=num_classes, pretrained=cfg[\"model\"][\"pretrained\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "model = maybe_compile(model)\n",
    "print(f\"Model created: {cfg['model']['architecture']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B初期化\n",
    "run = wandb.init(\n",
    "    project=cfg[\"logging\"][\"wandb\"][\"project\"],\n",
    "    name=cfg[\"logging\"][\"wandb\"][\"name\"],\n",
    "    config=cfg,\n",
    "    tags=cfg[\"logging\"][\"wandb\"][\"tags\"],\n",
    "    notes=cfg[\"logging\"][\"wandb\"][\"notes\"],\n",
    ")\n",
    "\n",
    "# W&B URL保存\n",
    "wandb_url = f\"https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\"\n",
    "with open(\"wandb_run.txt\", \"w\") as f:\n",
    "    f.write(f\"URL: {wandb_url}\\n\")\n",
    "    f.write(f\"Run ID: {wandb.run.id}\\n\")\n",
    "\n",
    "print(f\"W&B run: {wandb_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・評価関数（14ラベル対応, weighted AUC）\n",
    "def train_epoch(model, dataloader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(**autocast_kwargs()):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        if cfg[\"environment\"][\"mixed_precision\"]:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds_all, targs_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with autocast(**autocast_kwargs()):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds_all.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            targs_all.append(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    preds = np.concatenate(preds_all, axis=0)\n",
    "    targs = np.concatenate(targs_all, axis=0)\n",
    "    aucs = []\n",
    "    for j in range(preds.shape[1]):\n",
    "        y_true = targs[:, j]\n",
    "        y_pred = preds[:, j]\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            aucs.append(np.nan)\n",
    "        else:\n",
    "            aucs.append(roc_auc_score(y_true, y_pred))\n",
    "    aucs = np.array(aucs, dtype=float)\n",
    "    weights = np.array([13] + [1]*13, dtype=float)[: preds.shape[1]]\n",
    "    valid = ~np.isnan(aucs)\n",
    "    weighted_auc = np.average(aucs[valid], weights=weights[valid]) if valid.any() else np.nan\n",
    "    return avg_loss, float(weighted_auc), preds, targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation（14ラベル対応）\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros((len(train_df), num_classes), dtype=np.float32)\n",
    "oof_targets = train_df[label_cols].values.astype(\"float32\")\n",
    "\n",
    "# モデル保存ディレクトリ作成\n",
    "Path(\"model\").mkdir(exist_ok=True)\n",
    "\n",
    "for fold in range(cfg[\"cv\"][\"n_folds\"]):\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"FOLD {fold + 1}/{cfg['cv']['n_folds']}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # データ分割\n",
    "    train_fold_df = train_df[train_df[\"fold\"] != fold]\n",
    "    valid_fold_df = train_df[train_df[\"fold\"] == fold]\n",
    "\n",
    "    print(f\"Train: {len(train_fold_df)}, Valid: {len(valid_fold_df)}\")\n",
    "\n",
    "    # Dataset・DataLoader作成（label_cols を渡す）\n",
    "    train_dataset = AneurysmDataset(train_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"train\"), label_cols=label_cols)\n",
    "    valid_dataset = AneurysmDataset(valid_fold_df, cfg[\"paths\"][\"processed_data\"], transform=get_transforms(\"valid\"), label_cols=label_cols)\n",
    "\n",
    "    # DataLoader: Colab向けに調整（安定化）\n",
    "    num_workers = min(2, os.cpu_count() or 2)\n",
    "    use_pin = torch.cuda.is_available()\n",
    "    use_persist = False  # Colab は persistent_workers で落ちやすい\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_pin,\n",
    "        persistent_workers=use_persist,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_pin,\n",
    "        persistent_workers=use_persist,\n",
    "    )\n",
    "\n",
    "    # モデル・最適化設定\n",
    "    model = AneurysmModel(\n",
    "        model_name=cfg[\"model\"][\"architecture\"], num_classes=num_classes, pretrained=cfg[\"model\"][\"pretrained\"]\n",
    "    ).to(device)\n",
    "    model = maybe_compile(model)\n",
    "\n",
    "    optimizer_type = cfg[\"train\"][\"optimizer\"][\"type\"]\n",
    "    if optimizer_type == \"AdamW\":\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=cfg[\"train\"][\"optimizer\"][\"lr\"], weight_decay=cfg[\"train\"][\"optimizer\"][\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=cfg[\"environment\"][\"mixed_precision\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=cfg[\"train\"][\"scheduler\"][\"T_0\"], eta_min=cfg[\"train\"][\"scheduler\"][\"eta_min\"]\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    best_auc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg[\"train\"][\"epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
    "        valid_loss, valid_auc, valid_preds, valid_targets = validate_epoch(model, valid_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{cfg['train']['epochs']}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid AUC: {valid_auc:.4f}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"fold_{fold}_train_loss\": train_loss,\n",
    "                f\"fold_{fold}_valid_loss\": valid_loss,\n",
    "                f\"fold_{fold}_valid_auc\": valid_auc,\n",
    "                \"epoch\": epoch,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"model/fold_{fold}_best.pth\")\n",
    "            best_preds = valid_preds.copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= cfg[\"train\"][\"early_stopping\"][\"patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # OOF予測保存（行列）\n",
    "    valid_indices = valid_fold_df.index\n",
    "    oof_predictions[valid_indices] = best_preds\n",
    "\n",
    "    fold_scores.append(best_auc)\n",
    "    print(f\"Fold {fold + 1} Best AUC: {best_auc:.4f}\")\n",
    "\n",
    "    # メモリクリア\n",
    "    del model, optimizer, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# CV結果集計\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "\n",
    "# OOF weighted AUC（列ごとAUCの重み付き平均）\n",
    "aucs = []\n",
    "for j in range(oof_predictions.shape[1]):\n",
    "    y_true = oof_targets[:, j]\n",
    "    y_pred = oof_predictions[:, j]\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        aucs.append(np.nan)\n",
    "    else:\n",
    "        aucs.append(roc_auc_score(y_true, y_pred))\n",
    "aucs = np.array(aucs, dtype=float)\n",
    "weights = np.array([13] + [1]*13, dtype=float)[: oof_predictions.shape[1]]\n",
    "valid = ~np.isnan(aucs)\n",
    "oof_auc = np.average(aucs[valid], weights=weights[valid]) if valid.any() else np.nan\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"CV RESULTS\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Fold Scores: {fold_scores}\")\n",
    "print(f\"CV Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"OOF AUC (weighted): {oof_auc:.4f}\")\n",
    "\n",
    "wandb.log({\"cv_mean_auc\": cv_mean, \"cv_std_auc\": cv_std, \"oof_auc\": float(oof_auc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF結果保存（14ラベル、列名付き）\n",
    "oof_df = pd.DataFrame({\"index\": train_df.index, \"fold\": train_df[\"fold\"]})\n",
    "for i, c in enumerate(label_cols):\n",
    "    oof_df[f\"y_true_{c}\"] = oof_targets[:, i]\n",
    "    oof_df[f\"y_pred_{c}\"] = oof_predictions[:, i]\n",
    "oof_df.to_csv(\"oof_predictions.csv\", index=False)\n",
    "\n",
    "# メトリクス保存\n",
    "metrics = {\n",
    "    \"experiment_id\": cfg[\"experiment\"][\"id\"],\n",
    "    \"cv_mean_auc\": float(cv_mean),\n",
    "    \"cv_std_auc\": float(cv_std),\n",
    "    \"oof_auc\": float(oof_auc),\n",
    "    \"fold_scores\": [float(x) for x in fold_scores],\n",
    "    \"git_sha\": git_sha,\n",
    "    \"wandb_url\": wandb_url,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(\"- oof_predictions.csv\")\n",
    "print(\"- metrics.json\")\n",
    "print(f\"- model/fold_*_best.pth ({cfg['cv']['n_folds']} files)\")\n",
    "print(\"- wandb_run.txt\")\n",
    "print(\"- git_sha.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive に成果物バックアップ\n",
    "backup_dir = f\"/content/drive/MyDrive/rsna-aneurysm/{cfg['experiment']['id']}\"\n",
    "!mkdir -p \"{backup_dir}\"\n",
    "!cp oof_predictions.csv \"{backup_dir}/\"\n",
    "!cp metrics.json \"{backup_dir}/\"\n",
    "!cp -r model \"{backup_dir}/\"\n",
    "!cp wandb_run.txt \"{backup_dir}/\"\n",
    "!cp git_sha.txt \"{backup_dir}/\"\n",
    "\n",
    "print(f\"Results backed up to: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B終了\n",
    "wandb.finish()\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
