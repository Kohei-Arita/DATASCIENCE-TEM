{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Aneurysm Detection Evaluation - exp0001\n",
    "\n",
    "**ÁõÆÁöÑ**: Out-Of-Fold (OOF) ‰∫àÊ∏¨„ÅÆÂàÜÊûê„Å®ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "\n",
    "**ÂàÜÊûêÂÜÖÂÆπ**:\n",
    "- CVÊÄßËÉΩ„ÅÆË©≥Á¥∞ÂàÜÊûê\n",
    "- FoldÈñì„ÅÆ‰∏ÄË≤´ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "- „É™„Éº„ÇØÊ§úÂá∫„ÉªÂìÅË≥™Áõ£Êüª\n",
    "- ÈñæÂÄ§ÊúÄÈÅ©Âåñ\n",
    "- ÁâπÂæ¥ÈáçË¶ÅÂ∫¶ÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âü∫Êú¨„É©„Ç§„Éñ„É©„É™\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Ë©ï‰æ°ÊåáÊ®ô\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix,\n",
    "    classification_report, average_precision_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import scipy.stats as stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ë®≠ÂÆöË™≠„ÅøËæº„Åø\n",
    "with open('config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "print(f\"Evaluating experiment: {cfg['experiment']['id']}\")\n",
    "print(f\"Description: {cfg['experiment']['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁµêÊûú„Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "oof_df = pd.read_csv('oof_predictions.csv')\n",
    "with open('metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "    \n",
    "print(f\"OOF predictions shape: {oof_df.shape}\")\n",
    "print(f\"Metrics loaded: {list(metrics.keys())}\")\n",
    "print(f\"\\nOOF DataFrame:\")\n",
    "print(oof_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âü∫Êú¨Áµ±Ë®à„Çµ„Éû„É™„Éº\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"CV Mean AUC: {metrics['cv_mean_auc']:.6f} ¬± {metrics['cv_std_auc']:.6f}\")\n",
    "print(f\"OOF AUC: {metrics['oof_auc']:.6f}\")\n",
    "print(f\"\\nFold-wise AUC scores:\")\n",
    "for i, score in enumerate(metrics['fold_scores']):\n",
    "    print(f\"  Fold {i+1}: {score:.6f}\")\n",
    "\n",
    "# FoldÈñì„ÅÆ‰∏ÄË≤´ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "fold_std = np.std(metrics['fold_scores'])\n",
    "print(f\"\\nFold Score Standard Deviation: {fold_std:.6f}\")\n",
    "\n",
    "if fold_std > 0.02:\n",
    "    print(\"‚ö†Ô∏è  WARNING: High variance between folds (potential data leakage or instability)\")\n",
    "else:\n",
    "    print(\"‚úÖ Good: Consistent performance across folds\")\n",
    "\n",
    "# CV vs OOF‰πñÈõ¢„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "cv_oof_diff = abs(metrics['cv_mean_auc'] - metrics['oof_auc'])\n",
    "print(f\"\\nCV-OOF AUC Difference: {cv_oof_diff:.6f}\")\n",
    "\n",
    "if cv_oof_diff > 0.01:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Large discrepancy between CV and OOF AUC\")\n",
    "else:\n",
    "    print(\"‚úÖ Good: CV and OOF AUC are consistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FoldÂà•ÂàÜÊûê\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for fold in range(cfg['cv']['n_folds']):\n",
    "    fold_data = oof_df[oof_df['fold'] == fold]\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "    auc = roc_auc_score(fold_data['y_true'], fold_data['y_pred'])\n",
    "    \n",
    "    axes[fold].plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "    axes[fold].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[fold].set_xlabel('False Positive Rate')\n",
    "    axes[fold].set_ylabel('True Positive Rate')\n",
    "    axes[fold].set_title(f'ROC Curve - Fold {fold+1}')\n",
    "    axes[fold].legend()\n",
    "    axes[fold].grid(True, alpha=0.3)\n",
    "\n",
    "# ÂÖ®‰ΩìROC Curve\n",
    "fpr, tpr, _ = roc_curve(oof_df['y_true'], oof_df['y_pred'])\n",
    "auc = roc_auc_score(oof_df['y_true'], oof_df['y_pred'])\n",
    "\n",
    "axes[5].plot(fpr, tpr, label=f'Overall AUC = {auc:.4f}', linewidth=2)\n",
    "axes[5].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[5].set_xlabel('False Positive Rate')\n",
    "axes[5].set_ylabel('True Positive Rate')\n",
    "axes[5].set_title('Overall ROC Curve')\n",
    "axes[5].legend()\n",
    "axes[5].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∫àÊ∏¨ÂàÜÂ∏ÉÂàÜÊûê\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ‰∫àÊ∏¨Á¢∫ÁéáÂàÜÂ∏ÉÔºà„ÇØ„É©„ÇπÂà•Ôºâ\n",
    "axes[0,0].hist(oof_df[oof_df['y_true']==0]['y_pred'], bins=50, alpha=0.7, label='Negative', density=True)\n",
    "axes[0,0].hist(oof_df[oof_df['y_true']==1]['y_pred'], bins=50, alpha=0.7, label='Positive', density=True)\n",
    "axes[0,0].set_xlabel('Predicted Probability')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "axes[0,0].set_title('Prediction Distribution by Class')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# FoldÂà•‰∫àÊ∏¨ÂàÜÂ∏É\n",
    "for fold in range(cfg['cv']['n_folds']):\n",
    "    fold_data = oof_df[oof_df['fold'] == fold]\n",
    "    axes[0,1].hist(fold_data['y_pred'], bins=30, alpha=0.5, label=f'Fold {fold+1}', density=True)\n",
    "axes[0,1].set_xlabel('Predicted Probability')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Prediction Distribution by Fold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(oof_df['y_true'], oof_df['y_pred'])\n",
    "avg_precision = average_precision_score(oof_df['y_true'], oof_df['y_pred'])\n",
    "axes[1,0].plot(recall, precision, label=f'Average Precision = {avg_precision:.4f}')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].set_title('Precision-Recall Curve')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calibration Curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    oof_df['y_true'], oof_df['y_pred'], n_bins=10\n",
    ")\n",
    "axes[1,1].plot(mean_predicted_value, fraction_of_positives, \"s-\", label='Model')\n",
    "axes[1,1].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "axes[1,1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1,1].set_ylabel('Fraction of Positives')\n",
    "axes[1,1].set_title('Calibration Curve')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊúÄÈÅ©ÈñæÂÄ§Ê±∫ÂÆö\n",
    "def find_optimal_threshold(y_true, y_pred, method='youden'):\n",
    "    \"\"\"\n",
    "    ÊúÄÈÅ©ÈñæÂÄ§„ÇíË¶ã„Å§„Åë„Çã\n",
    "    method: 'youden', 'f1', 'precision_recall_balance'\n",
    "    \"\"\"\n",
    "    if method == 'youden':\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        optimal_idx = np.argmax(tpr - fpr)  # Youden's J statistic\n",
    "        return thresholds[optimal_idx]\n",
    "    \n",
    "    elif method == 'f1':\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        return thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    elif method == 'precision_recall_balance':\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "        balance_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        optimal_idx = np.argmax(balance_scores)\n",
    "        return thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "\n",
    "# ÂêÑÊâãÊ≥ï„ÅßÊúÄÈÅ©ÈñæÂÄ§Ë®àÁÆó\n",
    "thresholds = {}\n",
    "methods = ['youden', 'f1', 'precision_recall_balance']\n",
    "\n",
    "for method in methods:\n",
    "    threshold = find_optimal_threshold(oof_df['y_true'], oof_df['y_pred'], method)\n",
    "    thresholds[method] = threshold\n",
    "    \n",
    "    # Ë©≤ÂΩìÈñæÂÄ§„Åß„ÅÆÊÄßËÉΩË®àÁÆó\n",
    "    y_pred_binary = (oof_df['y_pred'] >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(oof_df['y_true'], y_pred_binary)\n",
    "    f1 = f1_score(oof_df['y_true'], y_pred_binary)\n",
    "    precision = precision_score(oof_df['y_true'], y_pred_binary)\n",
    "    recall = recall_score(oof_df['y_true'], y_pred_binary)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Method:\")\n",
    "    print(f\"  Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "\n",
    "# „Éá„Éï„Ç©„É´„ÉàÈñæÂÄ§(0.5)„Åß„ÅÆÊÄßËÉΩ\n",
    "y_pred_default = (oof_df['y_pred'] >= 0.5).astype(int)\n",
    "print(f\"\\nDEFAULT (0.5) Method:\")\n",
    "print(f\"  Threshold: 0.5000\")\n",
    "print(f\"  Accuracy:  {accuracy_score(oof_df['y_true'], y_pred_default):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(oof_df['y_true'], y_pred_default):.4f}\")\n",
    "print(f\"  Precision: {precision_score(oof_df['y_true'], y_pred_default):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(oof_df['y_true'], y_pred_default):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê∑∑ÂêåË°åÂàó (ÊúÄÈÅ©ÈñæÂÄ§‰ΩøÁî®)\n",
    "optimal_threshold = thresholds['youden']  # YoudenÊ≥ï„ÇíÊé°Áî®\n",
    "y_pred_optimal = (oof_df['y_pred'] >= optimal_threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(oof_df['y_true'], y_pred_optimal)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title(f'Confusion Matrix (Threshold: {optimal_threshold:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ë©≥Á¥∞ÂàÜÈ°û„É¨„Éù„Éº„Éà\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(oof_df['y_true'], y_pred_optimal, \n",
    "                          target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „É™„Éº„ÇØÊ§úÂá∫„ÉªÂìÅË≥™Áõ£Êüª\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA QUALITY AUDIT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. FoldÈñìTargetÂàÜÂ∏É‰∏ÄË≤´ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "fold_target_rates = []\n",
    "for fold in range(cfg['cv']['n_folds']):\n",
    "    fold_data = oof_df[oof_df['fold'] == fold]\n",
    "    target_rate = fold_data['y_true'].mean()\n",
    "    fold_target_rates.append(target_rate)\n",
    "    print(f\"Fold {fold+1} positive rate: {target_rate:.4f} ({fold_data['y_true'].sum()}/{len(fold_data)})\")\n",
    "\n",
    "target_rate_std = np.std(fold_target_rates)\n",
    "print(f\"\\nTarget rate std across folds: {target_rate_std:.6f}\")\n",
    "\n",
    "if target_rate_std > 0.05:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Inconsistent target distribution across folds\")\n",
    "else:\n",
    "    print(\"‚úÖ Good: Consistent target distribution across folds\")\n",
    "\n",
    "# 2. ‰∫àÊ∏¨Á¢∫Áéá„ÅÆÁï∞Â∏∏ÂÄ§„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "extreme_high = (oof_df['y_pred'] > 0.99).sum()\n",
    "extreme_low = (oof_df['y_pred'] < 0.01).sum()\n",
    "print(f\"\\nExtreme predictions:\")\n",
    "print(f\"  Very confident positive (>0.99): {extreme_high} samples\")\n",
    "print(f\"  Very confident negative (<0.01): {extreme_low} samples\")\n",
    "\n",
    "total_extreme = extreme_high + extreme_low\n",
    "extreme_ratio = total_extreme / len(oof_df)\n",
    "\n",
    "if extreme_ratio > 0.1:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {extreme_ratio:.2%} of predictions are extreme (potential overfitting)\")\n",
    "else:\n",
    "    print(f\"‚úÖ Good: Only {extreme_ratio:.2%} extreme predictions\")\n",
    "\n",
    "# 3. CV-Test Áõ∏Èñ¢„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ÔºàÂ∞ÜÊù•„ÅÆLBÊØîËºÉÁî®Ôºâ\n",
    "print(f\"\\n=== CV Reliability Metrics ===\")\n",
    "print(f\"Expected CV-LB correlation indicators:\")\n",
    "print(f\"  Fold score consistency: {1 - fold_std:.4f} (higher is better)\")\n",
    "print(f\"  CV-OOF agreement: {1 - cv_oof_diff:.4f} (higher is better)\")\n",
    "print(f\"  Target distribution stability: {1 - target_rate_std:.4f} (higher is better)\")\n",
    "\n",
    "cv_reliability_score = np.mean([1 - fold_std, 1 - cv_oof_diff, 1 - target_rate_std])\n",
    "print(f\"\\nüìä Overall CV Reliability Score: {cv_reliability_score:.4f}\")\n",
    "\n",
    "if cv_reliability_score > 0.9:\n",
    "    print(\"‚úÖ Excellent: High confidence in CV\")\n",
    "elif cv_reliability_score > 0.8:\n",
    "    print(\"‚úÖ Good: Reasonable confidence in CV\")\n",
    "elif cv_reliability_score > 0.7:\n",
    "    print(\"‚ö†Ô∏è  Fair: Some concerns about CV reliability\")\n",
    "else:\n",
    "    print(\"‚ùå Poor: Low confidence in CV - consider strategy revision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë©≥Á¥∞„É°„Éà„É™„ÇØ„Çπ‰øùÂ≠òÔºàevaluationÁî®Ôºâ\n",
    "evaluation_metrics = {\n",
    "    'experiment_id': cfg['experiment']['id'],\n",
    "    'evaluation_date': pd.Timestamp.now().isoformat(),\n",
    "    \n",
    "    # Âü∫Êú¨ÊÄßËÉΩÊåáÊ®ô\n",
    "    'oof_auc': float(metrics['oof_auc']),\n",
    "    'cv_mean_auc': float(metrics['cv_mean_auc']),\n",
    "    'cv_std_auc': float(metrics['cv_std_auc']),\n",
    "    'average_precision': float(average_precision_score(oof_df['y_true'], oof_df['y_pred'])),\n",
    "    \n",
    "    # ÊúÄÈÅ©ÈñæÂÄ§ÊÉÖÂ†±\n",
    "    'optimal_thresholds': {\n",
    "        method: float(threshold) for method, threshold in thresholds.items()\n",
    "    },\n",
    "    'recommended_threshold': float(optimal_threshold),\n",
    "    \n",
    "    # ÊúÄÈÅ©ÈñæÂÄ§„Åß„ÅÆÊÄßËÉΩ\n",
    "    'optimal_threshold_metrics': {\n",
    "        'accuracy': float(accuracy_score(oof_df['y_true'], y_pred_optimal)),\n",
    "        'f1': float(f1_score(oof_df['y_true'], y_pred_optimal)),\n",
    "        'precision': float(precision_score(oof_df['y_true'], y_pred_optimal)),\n",
    "        'recall': float(recall_score(oof_df['y_true'], y_pred_optimal))\n",
    "    },\n",
    "    \n",
    "    # ÂìÅË≥™Áõ£ÊüªÁµêÊûú\n",
    "    'quality_audit': {\n",
    "        'fold_consistency_score': float(1 - fold_std),\n",
    "        'cv_oof_agreement': float(1 - cv_oof_diff), \n",
    "        'target_distribution_stability': float(1 - target_rate_std),\n",
    "        'cv_reliability_score': float(cv_reliability_score),\n",
    "        'extreme_prediction_ratio': float(extreme_ratio)\n",
    "    },\n",
    "    \n",
    "    # FoldÂà•Ë©≥Á¥∞\n",
    "    'fold_details': {\n",
    "        f'fold_{i+1}': {\n",
    "            'auc': float(metrics['fold_scores'][i]),\n",
    "            'target_rate': float(fold_target_rates[i]),\n",
    "            'n_samples': int((oof_df['fold'] == i).sum())\n",
    "        } for i in range(cfg['cv']['n_folds'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ë©ï‰æ°ÁµêÊûú‰øùÂ≠ò\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nüìÅ Evaluation results saved:\")\n",
    "print(\"- evaluation_metrics.json\")\n",
    "print(\"- roc_curves_analysis.png\")\n",
    "print(\"- prediction_analysis.png\")\n",
    "print(\"- confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆüÈ®ì„Éé„Éº„ÉàÊõ¥Êñ∞Áî®„ÅÆ„Çµ„Éû„É™„ÉºÁîüÊàê\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY FOR NOTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_text = f\"\"\"## Experiment {cfg['experiment']['id']} - Results Summary\n",
    "\n",
    "**Model**: {cfg['model']['architecture']}\n",
    "**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "### Performance\n",
    "- CV AUC: {metrics['cv_mean_auc']:.6f} ¬± {metrics['cv_std_auc']:.6f}\n",
    "- OOF AUC: {metrics['oof_auc']:.6f}\n",
    "- Average Precision: {average_precision_score(oof_df['y_true'], oof_df['y_pred']):.6f}\n",
    "\n",
    "### Optimal Threshold: {optimal_threshold:.4f} (Youden method)\n",
    "- Accuracy: {accuracy_score(oof_df['y_true'], y_pred_optimal):.4f}\n",
    "- F1 Score: {f1_score(oof_df['y_true'], y_pred_optimal):.4f}\n",
    "- Precision: {precision_score(oof_df['y_true'], y_pred_optimal):.4f}\n",
    "- Recall: {recall_score(oof_df['y_true'], y_pred_optimal):.4f}\n",
    "\n",
    "### Quality Assessment\n",
    "- CV Reliability Score: {cv_reliability_score:.4f}\n",
    "- Fold Consistency: {'‚úÖ Good' if fold_std <= 0.02 else '‚ö†Ô∏è Concerning'}\n",
    "- CV-OOF Agreement: {'‚úÖ Good' if cv_oof_diff <= 0.01 else '‚ö†Ô∏è Concerning'}\n",
    "\n",
    "### Key Observations\n",
    "- {\"Stable cross-validation with consistent performance across folds\" if fold_std <= 0.02 else \"High variance between folds - investigate potential issues\"}\n",
    "- {\"Well-calibrated model with good probability estimates\" if cv_oof_diff <= 0.01 else \"Model calibration may need improvement\"}\n",
    "- Extreme predictions: {extreme_ratio:.2%} of samples\n",
    "\n",
    "### Recommendations for Next Experiments\n",
    "1. {'Try different model architectures' if metrics['oof_auc'] < 0.8 else 'Focus on ensemble methods and fine-tuning'}\n",
    "2. {'Investigate data augmentation strategies' if 'light' in cfg.get('augmentation', {}).get('strength', 'light') else 'Consider regularization techniques'}\n",
    "3. {'Review CV strategy if reliability issues persist' if cv_reliability_score < 0.8 else 'Current CV strategy appears reliable'}\n",
    "\"\"\"\n",
    "\n",
    "# notes.md„Éï„Ç°„Ç§„É´„Å´ËøΩË®ò\n",
    "with open('notes.md', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "    \n",
    "print(summary_text)\n",
    "print(\"\\nüìù Summary saved to notes.md\")\n",
    "\n",
    "print(\"\\nüéØ Evaluation completed successfully!\")\n",
    "print(\"Next step: Run inference.ipynb for test predictions and submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}