{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Aneurysm Detection Inference - exp0001\n",
    "\n",
    "**ç›®çš„**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ¨è«–ã¨Kaggleæå‡º\n",
    "\n",
    "**å®Ÿè¡Œå†…å®¹**:\n",
    "- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¨è«–ï¼ˆTTAé©ç”¨ï¼‰\n",
    "- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n",
    "- é–¾å€¤é©ç”¨ãƒ»æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\n",
    "- Kaggle APIè‡ªå‹•æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "# PyTorché–¢é€£\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# ç”»åƒå‡¦ç†\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# é€²æ—ãƒãƒ¼\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# è¨­å®šèª­ã¿è¾¼ã¿\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "with open(\"evaluation_metrics.json\", \"r\") as f:\n",
    "    eval_metrics = json.load(f)\n",
    "\n",
    "print(f\"Running inference for: {cfg['experiment']['id']}\")\n",
    "print(f\"Model: {cfg['model']['architecture']}\")\n",
    "print(f\"Recommended threshold: {eval_metrics['recommended_threshold']:.4f}\")\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(cfg[\"environment\"][\"device\"])\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ï¼‰\n",
    "class AneurysmModel(nn.Module):\n",
    "    def __init__(self, model_name=\"resnet50\", num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if model_name == \"resnet50\":\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(in_features, cfg[\"model\"][\"hidden_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg[\"model\"][\"dropout\"]),\n",
    "            nn.Linear(cfg[\"model\"][\"hidden_dim\"], num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "print(\"Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "models_list = []\n",
    "for fold in range(cfg[\"cv\"][\"n_folds\"]):\n",
    "    model_path = f\"model/fold_{fold}_best.pth\"\n",
    "    if Path(model_path).exists():\n",
    "        model = AneurysmModel(\n",
    "            model_name=cfg[\"model\"][\"architecture\"],\n",
    "            num_classes=cfg[\"model\"][\"num_classes\"],\n",
    "            pretrained=False,  # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã®ã§False\n",
    "        )\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models_list.append(model)\n",
    "        print(f\"Loaded model for fold {fold + 1}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Model file not found: {model_path}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models_list)}\")\n",
    "if len(models_list) == 0:\n",
    "    raise FileNotFoundError(\"No model files found! Run training.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆä»®ã®ãƒ‘ã‚¹ - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦ä¿®æ­£ï¼‰\n",
    "data_dir = Path(cfg[\"paths\"][\"data_dir\"])\n",
    "test_df = pd.read_csv(data_dir / \"test.csv\")  # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {list(test_df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨Transformå®šç¾©ï¼ˆTTAç”¨ã®è¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰\n",
    "def get_test_transforms(tta_type=None):\n",
    "    base_transforms = [\n",
    "        A.Resize(cfg[\"data\"][\"image_size\"][0], cfg[\"data\"][\"image_size\"][1]),\n",
    "    ]\n",
    "\n",
    "    # TTA transforms\n",
    "    if tta_type == \"hflip\":\n",
    "        base_transforms.append(A.HorizontalFlip(p=1.0))\n",
    "    elif tta_type == \"vflip\":\n",
    "        base_transforms.append(A.VerticalFlip(p=1.0))\n",
    "    elif tta_type == \"rotate\":\n",
    "        base_transforms.append(A.Rotate(limit=10, p=1.0))\n",
    "\n",
    "    base_transforms.extend(\n",
    "        [\n",
    "            A.Normalize(\n",
    "                mean=cfg[\"data\"][\"normalization\"][\"mean\"], std=cfg[\"data\"][\"normalization\"][\"std\"], max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return A.Compose(base_transforms)\n",
    "\n",
    "\n",
    "# TTAè¨­å®š\n",
    "tta_transforms = [None, \"hflip\", \"vflip\"] if cfg[\"model\"].get(\"inference\", {}).get(\"tta_enabled\", True) else [None]\n",
    "print(f\"TTA transforms: {tta_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨Dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ç”»åƒèª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒ»å‘½åè¦å‰‡ã«å¿œã˜ã¦ä¿®æ­£ï¼‰\n",
    "        image_path = self.image_dir / f\"{row['image_id']}.png\"\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "print(\"Test dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–å®Ÿè¡Œ\n",
    "def predict_with_tta(models, dataloader, device, use_amp=True):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« + TTA ã§äºˆæ¸¬\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            images = batch.to(device)\n",
    "            batch_predictions = []\n",
    "\n",
    "            # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬\n",
    "            for model in models:\n",
    "                with autocast(enabled=use_amp):\n",
    "                    outputs = model(images)\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    batch_predictions.append(probs)\n",
    "\n",
    "            # ãƒ¢ãƒ‡ãƒ«é–“å¹³å‡\n",
    "            ensemble_preds = np.mean(batch_predictions, axis=0)\n",
    "            all_predictions.extend(ensemble_preds.flatten())\n",
    "\n",
    "    return np.array(all_predictions)\n",
    "\n",
    "\n",
    "# TTAæ¨è«–å®Ÿè¡Œ\n",
    "tta_predictions = []\n",
    "\n",
    "for tta_type in tta_transforms:\n",
    "    print(f\"\\nRunning inference with TTA: {tta_type}\")\n",
    "\n",
    "    # Datasetãƒ»DataLoaderä½œæˆ\n",
    "    test_dataset = TestDataset(\n",
    "        test_df,\n",
    "        cfg[\"paths\"][\"processed_data\"],  # ãƒ†ã‚¹ãƒˆç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        transform=get_test_transforms(tta_type),\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg[\"train\"][\"batch_size\"] * 2,  # æ¨è«–æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºå¤§ãã‚\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # äºˆæ¸¬å®Ÿè¡Œ\n",
    "    predictions = predict_with_tta(models_list, test_loader, device, use_amp=cfg[\"environment\"][\"mixed_precision\"])\n",
    "\n",
    "    tta_predictions.append(predictions)\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "# TTAçµæœã®å¹³å‡\n",
    "final_predictions = np.mean(tta_predictions, axis=0)\n",
    "print(f\"\\nğŸ¯ Final predictions (TTA ensemble):\")\n",
    "print(f\"  Shape: {final_predictions.shape}\")\n",
    "print(f\"  Range: [{final_predictions.min():.4f}, {final_predictions.max():.4f}]\")\n",
    "print(f\"  Mean: {final_predictions.mean():.4f}\")\n",
    "print(f\"  Std: {final_predictions.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬åˆ†å¸ƒã®å¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(final_predictions, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "plt.axvline(\n",
    "    eval_metrics[\"recommended_threshold\"],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Recommended threshold: {eval_metrics['recommended_threshold']:.3f}\",\n",
    ")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Test Predictions Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ç´¯ç©åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_preds = np.sort(final_predictions)\n",
    "cumulative = np.arange(1, len(sorted_preds) + 1) / len(sorted_preds)\n",
    "plt.plot(sorted_preds, cumulative)\n",
    "plt.axvline(eval_metrics[\"recommended_threshold\"], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"Cumulative Distribution\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test_predictions_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# äºˆæ¸¬çµ±è¨ˆ\n",
    "threshold = eval_metrics[\"recommended_threshold\"]\n",
    "positive_predictions = (final_predictions >= threshold).sum()\n",
    "positive_ratio = positive_predictions / len(final_predictions)\n",
    "\n",
    "print(f\"\\nğŸ“Š Test Predictions Summary:\")\n",
    "print(f\"  Total samples: {len(final_predictions)}\")\n",
    "print(f\"  Predicted positive (â‰¥{threshold:.3f}): {positive_predictions} ({positive_ratio:.2%})\")\n",
    "print(f\"  Predicted negative (<{threshold:.3f}): {len(final_predictions) - positive_predictions} ({1 - positive_ratio:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "def create_submission_file(test_df, predictions, threshold, output_dir=\"submissions\"):\n",
    "    \"\"\"\n",
    "    Kaggleæå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # äºˆæ¸¬ç¢ºç‡ç‰ˆï¼ˆä¸»è¦æå‡ºï¼‰\n",
    "    submission_proba = test_df[[\"image_id\"]].copy()\n",
    "    submission_proba[\"aneurysm\"] = predictions  # ç¢ºç‡å€¤ã‚’ãã®ã¾ã¾\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    proba_filename = f\"{output_dir}/{cfg['experiment']['id']}_proba_{timestamp}.csv\"\n",
    "    submission_proba.to_csv(proba_filename, index=False)\n",
    "\n",
    "    # äºŒå€¤äºˆæ¸¬ç‰ˆï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰\n",
    "    submission_binary = test_df[[\"image_id\"]].copy()\n",
    "    submission_binary[\"aneurysm\"] = (predictions >= threshold).astype(int)\n",
    "\n",
    "    binary_filename = f\"{output_dir}/{cfg['experiment']['id']}_binary_th{threshold:.3f}_{timestamp}.csv\"\n",
    "    submission_binary.to_csv(binary_filename, index=False)\n",
    "\n",
    "    return proba_filename, binary_filename\n",
    "\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\n",
    "proba_file, binary_file = create_submission_file(test_df, final_predictions, eval_metrics[\"recommended_threshold\"])\n",
    "\n",
    "print(f\"\\nğŸ“„ Submission files created:\")\n",
    "print(f\"  Probability version: {proba_file}\")\n",
    "print(f\"  Binary version: {binary_file}\")\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª\n",
    "submission_df = pd.read_csv(proba_file)\n",
    "print(f\"\\nğŸ“‹ Submission file preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(submission_df[\"aneurysm\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle APIæå‡ºæº–å‚™\n",
    "def submit_to_kaggle(submission_file, competition_name, message):\n",
    "    \"\"\"\n",
    "    Kaggle APIã§æå‡º\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cmd = f'kaggle competitions submit -c {competition_name} -f {submission_file} -m \"{message}\"'\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Successfully submitted to Kaggle!\")\n",
    "            print(f\"Output: {result.stdout}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Submission failed: {result.stderr}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during submission: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# æå‡ºå®Ÿè¡Œ\n",
    "git_sha = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()[:8]\n",
    "submission_message = cfg[\"kaggle\"][\"submission_message\"].format(git_sha=git_sha)\n",
    "\n",
    "print(f\"\\nğŸš€ Submitting to Kaggle...\")\n",
    "print(f\"Competition: {cfg['kaggle']['competition']}\")\n",
    "print(f\"File: {proba_file}\")\n",
    "print(f\"Message: {submission_message}\")\n",
    "\n",
    "# å®Ÿéš›ã®æå‡ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã‚‹å ´åˆã¯æ‰‹å‹•ã§å®Ÿè¡Œï¼‰\n",
    "success = submit_to_kaggle(proba_file, cfg[\"kaggle\"][\"competition\"], submission_message)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nğŸ† Submission completed!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Manual submission required:\")\n",
    "    print(f\"   kaggle competitions submit -c {cfg['kaggle']['competition']} -f {proba_file} -m '{submission_message}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–çµæœä¿å­˜ï¼ˆãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£ç”¨ï¼‰\n",
    "inference_manifest = {\n",
    "    \"experiment_id\": cfg[\"experiment\"][\"id\"],\n",
    "    \"inference_date\": datetime.now().isoformat(),\n",
    "    \"git_sha\": git_sha,\n",
    "    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±\n",
    "    \"models_used\": [f\"fold_{i}_best.pth\" for i in range(len(models_list))],\n",
    "    \"model_architecture\": cfg[\"model\"][\"architecture\"],\n",
    "    # æ¨è«–è¨­å®š\n",
    "    \"tta_enabled\": len(tta_transforms) > 1,\n",
    "    \"tta_methods\": tta_transforms,\n",
    "    \"threshold_used\": float(eval_metrics[\"recommended_threshold\"]),\n",
    "    \"mixed_precision\": cfg[\"environment\"][\"mixed_precision\"],\n",
    "    # äºˆæ¸¬çµ±è¨ˆ\n",
    "    \"test_samples\": len(final_predictions),\n",
    "    \"prediction_statistics\": {\n",
    "        \"mean\": float(final_predictions.mean()),\n",
    "        \"std\": float(final_predictions.std()),\n",
    "        \"min\": float(final_predictions.min()),\n",
    "        \"max\": float(final_predictions.max()),\n",
    "        \"positive_predictions\": int(positive_predictions),\n",
    "        \"positive_ratio\": float(positive_ratio),\n",
    "    },\n",
    "    # æå‡ºæƒ…å ±\n",
    "    \"submission_files\": {\"probability\": proba_file, \"binary\": binary_file},\n",
    "    \"kaggle_submission\": {\"competition\": cfg[\"kaggle\"][\"competition\"], \"message\": submission_message, \"success\": success},\n",
    "    # å“è³ªæŒ‡æ¨™ï¼ˆå­¦ç¿’æ™‚å‚ç…§ï¼‰\n",
    "    \"training_performance\": {\n",
    "        \"cv_auc\": eval_metrics[\"cv_mean_auc\"],\n",
    "        \"oof_auc\": eval_metrics[\"oof_auc\"],\n",
    "        \"cv_reliability_score\": eval_metrics[\"quality_audit\"][\"cv_reliability_score\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆä¿å­˜\n",
    "with open(\"inference_manifest.json\", \"w\") as f:\n",
    "    json.dump(inference_manifest, f, indent=2)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚‚ä¿å­˜ï¼ˆå¾Œã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ä½¿ç”¨å¯èƒ½ï¼‰\n",
    "test_predictions_df = test_df[[\"image_id\"]].copy()\n",
    "test_predictions_df[\"predicted_probability\"] = final_predictions\n",
    "test_predictions_df[\"experiment_id\"] = cfg[\"experiment\"][\"id\"]\n",
    "test_predictions_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Inference artifacts saved:\")\n",
    "print(f\"  - inference_manifest.json\")\n",
    "print(f\"  - test_predictions.csv\")\n",
    "print(f\"  - test_predictions_distribution.png\")\n",
    "print(f\"  - {proba_file}\")\n",
    "print(f\"  - {binary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
    "backup_dir = f\"/content/drive/MyDrive/rsna-aneurysm/{cfg['experiment']['id']}\"\n",
    "!mkdir -p \"{backup_dir}/submissions\"\n",
    "\n",
    "# é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Driveã«ã‚³ãƒ”ãƒ¼\n",
    "!cp inference_manifest.json \"{backup_dir}/\"\n",
    "!cp test_predictions.csv \"{backup_dir}/\"\n",
    "!cp test_predictions_distribution.png \"{backup_dir}/\"\n",
    "!cp \"{proba_file}\" \"{backup_dir}/submissions/\"\n",
    "!cp \"{binary_file}\" \"{backup_dir}/submissions/\"\n",
    "\n",
    "print(f\"\\nâ˜ï¸  Files backed up to Google Drive: {backup_dir}\")\n",
    "\n",
    "# å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ¯ EXPERIMENT {cfg['experiment']['id'].upper()} - INFERENCE COMPLETED\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Model: {cfg['model']['architecture']}\")\n",
    "print(f\"CV Performance: {eval_metrics['cv_mean_auc']:.4f} Â± {eval_metrics['cv_std_auc']:.4f}\")\n",
    "print(f\"Test Predictions: {len(final_predictions)} samples\")\n",
    "print(f\"Predicted Positive: {positive_predictions} ({positive_ratio:.2%})\")\n",
    "print(f\"Kaggle Submission: {'âœ… Success' if success else 'âš ï¸ Manual required'}\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Monitor Kaggle LB score\")\n",
    "print(f\"  2. Update experiments.csv with LB results\")\n",
    "print(f\"  3. Plan next experiment (exp0002)\")\n",
    "print(f\"\\nğŸš€ Ready for next experiment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}