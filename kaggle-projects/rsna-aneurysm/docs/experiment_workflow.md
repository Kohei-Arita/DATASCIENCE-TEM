# å®Ÿé¨“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¬ã‚¤ãƒ‰ - RSNA Aneurysm Detection

## ğŸ”„ å®Ÿé¨“ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æ¦‚è¦

```mermaid
graph TD
    A[ä»®èª¬ãƒ»ã‚¢ã‚¤ãƒ‡ã‚¢] --> B[å®Ÿé¨“è¨­è¨ˆ]
    B --> C[å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ]
    C --> D[è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª¿æ•´]
    D --> E[å­¦ç¿’å®Ÿè¡Œ]
    E --> F[è©•ä¾¡ãƒ»åˆ†æ]
    F --> G[æ¨è«–ãƒ»æå‡º]
    G --> H[çµæœè¨˜éŒ²]
    H --> I[æ¬¡å®Ÿé¨“è¨ˆç”»]
    I --> A
```

## 1ï¸âƒ£ å®Ÿé¨“è¨­è¨ˆãƒ»è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º

### 1.1 æ–°å®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢ã®æ•´ç†

```markdown
# å®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ»ä»®èª¬ã‚·ãƒ¼ãƒˆ

## å®Ÿé¨“ID: exp0002
## ä»®èª¬
ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã§ã€åŒ»ç™‚ç”»åƒã®æ±åŒ–æ€§èƒ½ãŒå‘ä¸Šã—ã€CV AUC 0.85+ ã‚’é”æˆã§ãã‚‹

## å¤‰æ›´ç‚¹
- CLAHEï¼ˆContrast Limited Adaptive Histogram Equalizationï¼‰ã‚’è¿½åŠ 
- Elastic Transform ã‚’è¿½åŠ   
- æ‹¡å¼µå¼·åº¦ã‚’ light â†’ medium ã«å¤‰æ›´

## æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ
- ç”»åƒã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæ”¹å–„ã«ã‚ˆã‚Šç‰¹å¾´æŠ½å‡ºã®ç²¾åº¦å‘ä¸Š
- å½¢çŠ¶å¤‰å½¢ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½ã®å‘ä¸Š
- CV AUC: 0.8300 â†’ 0.8500 (ç›®æ¨™)

## ãƒªã‚¹ã‚¯
- éåº¦ãªå¤‰å½¢ã«ã‚ˆã‚ŠåŒ»å­¦çš„ç‰¹å¾´ãŒå¤±ã‚ã‚Œã‚‹å¯èƒ½æ€§
- å­¦ç¿’æ™‚é–“ã®å¢—åŠ 

## æˆåŠŸåŸºæº–
- CV AUC > 0.84
- CV-LBç›¸é–¢ã®ç¶­æŒ (r > 0.7)
- Foldé–“ã‚¹ã‚³ã‚¢æ¨™æº–åå·® < 0.02
```

### 1.2 æ–°å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ

```bash
# æ–°å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir experiments/exp0002

# ãƒ™ãƒ¼ã‚¹å®Ÿé¨“ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
cp -r experiments/exp0001/{training.ipynb,evaluation.ipynb,inference.ipynb,env} experiments/exp0002/

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼ã¨èª¿æ•´
cp experiments/exp0001/config.yaml experiments/exp0002/config.yaml
```

### 1.3 è¨­å®šå·®åˆ†ã®æ±ºå®š

```yaml
# experiments/exp0002/config.yaml
experiment:
  id: exp0002
  description: "Enhanced augmentation with CLAHE and elastic transforms"
  hypothesis: "Stronger augmentation improves generalization for medical images"

augmentation:
  strength: "medium"  # light â†’ medium
  train_transforms:
    - CLAHE: {clip_limit: 3.0, tile_grid_size: [8, 8], p: 0.5}  # æ–°è¦è¿½åŠ 
    - ElasticTransform: {alpha: 30, sigma: 5, p: 0.3}  # æ–°è¦è¿½åŠ 
    # æ—¢å­˜ã®è¨­å®šã¯ç¶™æ‰¿
```

## 2ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»CVåˆ†å‰²å›ºå®šãƒ•ã‚§ãƒ¼ã‚º

### 2.1 äº¤å·®æ¤œè¨¼åˆ†å‰²ã®ä½œæˆ

```python
# CVåˆ†å‰²ã‚’ä½œæˆãƒ»å›ºå®š
from sklearn.model_selection import StratifiedGroupKFold
import pandas as pd

def create_cv_splits(train_df, config):
    """CVåˆ†å‰²ã‚’ä½œæˆã—ã¦ä¿å­˜"""
    
    sgkf = StratifiedGroupKFold(
        n_splits=config['cv']['n_folds'],
        shuffle=True,
        random_state=config['cv']['seed']
    )
    
    train_df['fold'] = -1
    for fold, (train_idx, val_idx) in enumerate(
        sgkf.split(
            train_df,
            train_df[config['data']['target_column']],
            train_df[config['cv']['group_column']]
        )
    ):
        train_df.loc[val_idx, 'fold'] = fold
    
    # CVåˆ†å‰²ä¿å­˜
    cv_splits_df = train_df[['PatientID', 'fold', config['data']['target_column']]].copy()
    cv_splits_df.to_csv('cv_folds.csv', index=False)
    
    return train_df

# åˆ†å‰²å“è³ªãƒã‚§ãƒƒã‚¯
def validate_cv_splits(train_df, target_col, group_col):
    """CVåˆ†å‰²ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
    
    fold_stats = []
    for fold in range(5):
        fold_data = train_df[train_df['fold'] == fold]
        
        stats = {
            'fold': fold,
            'n_samples': len(fold_data),
            'n_patients': fold_data[group_col].nunique(),
            'positive_rate': fold_data[target_col].mean(),
            'n_positive': fold_data[target_col].sum()
        }
        fold_stats.append(stats)
    
    fold_df = pd.DataFrame(fold_stats)
    
    # å“è³ªãƒã‚§ãƒƒã‚¯
    pos_rate_std = fold_df['positive_rate'].std()
    sample_size_cv = fold_df['n_samples'].std() / fold_df['n_samples'].mean()
    
    print("CV Splits Quality Check:")
    print(f"Positive rate std: {pos_rate_std:.4f} (<0.05 is good)")
    print(f"Sample size CV: {sample_size_cv:.4f} (<0.1 is good)")
    
    return fold_df
```

### 2.2 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£æŸ»

```python
def audit_data_quality(train_df, test_df):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªç›£æŸ»"""
    
    print("Data Quality Audit")
    print("=" * 50)
    
    # åŸºæœ¬çµ±è¨ˆ
    print(f"Training samples: {len(train_df)}")
    print(f"Test samples: {len(test_df)}")
    print(f"Positive rate: {train_df['aneurysm'].mean():.4f}")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    train_duplicates = train_df.duplicated().sum()
    test_duplicates = test_df.duplicated().sum()
    print(f"Training duplicates: {train_duplicates}")
    print(f"Test duplicates: {test_duplicates}")
    
    # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    train_missing = train_df.isnull().sum()
    test_missing = test_df.isnull().sum()
    print(f"Training missing values:\n{train_missing[train_missing > 0]}")
    print(f"Test missing values:\n{test_missing[test_missing > 0]}")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒæ¯”è¼ƒï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    common_cols = set(train_df.columns) & set(test_df.columns) - {'aneurysm'}
    for col in common_cols:
        if train_df[col].dtype in ['int64', 'float64']:
            train_mean = train_df[col].mean()
            test_mean = test_df[col].mean()
            diff = abs(train_mean - test_mean) / train_mean if train_mean != 0 else 0
            if diff > 0.1:  # 10%ä»¥ä¸Šã®å·®
                print(f"âš ï¸ Distribution shift in {col}: train={train_mean:.3f}, test={test_mean:.3f}")
```

## 3ï¸âƒ£ å­¦ç¿’å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºï¼ˆtraining.ipynbï¼‰

### 3.1 å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²

```python
import wandb
import subprocess
from datetime import datetime

def initialize_experiment(config):
    """å®Ÿé¨“åˆæœŸåŒ–"""
    
    # Git SHAè¨˜éŒ²
    git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()
    
    # W&BåˆæœŸåŒ–
    run = wandb.init(
        project=config['logging']['wandb']['project'],
        name=config['logging']['wandb']['name'],
        config=config,
        tags=config['logging']['wandb']['tags'],
        notes=config['experiment']['hypothesis']
    )
    
    # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    experiment_metadata = {
        'experiment_id': config['experiment']['id'],
        'start_time': datetime.now().isoformat(),
        'git_sha': git_sha,
        'git_branch': subprocess.check_output(['git', 'branch', '--show-current']).decode().strip(),
        'wandb_run_id': run.id,
        'wandb_url': f"https://wandb.ai/{run.entity}/{run.project}/runs/{run.id}",
        'hypothesis': config['experiment']['hypothesis'],
        'config_snapshot': config
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open('git_sha.txt', 'w') as f:
        f.write(git_sha)
    
    with open('wandb_run.txt', 'w') as f:
        f.write(f"URL: {experiment_metadata['wandb_url']}\n")
        f.write(f"Run ID: {run.id}\n")
    
    return run, experiment_metadata
```

### 3.2 å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
def train_with_monitoring(model, train_loader, valid_loader, config):
    """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãå­¦ç¿’"""
    
    # Early Stoppingè¨­å®š
    early_stopping = EarlyStopping(
        patience=config['train']['early_stopping']['patience'],
        mode='max',
        min_delta=0.001
    )
    
    best_score = 0
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'val_auc': [],
        'learning_rate': []
    }
    
    for epoch in range(config['train']['epochs']):
        # å­¦ç¿’
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # æ¤œè¨¼
        val_loss, val_auc, val_preds, val_targets = validate_epoch(model, valid_loader, criterion, device)
        
        # å­¦ç¿’ç‡æ›´æ–°
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # å±¥æ­´è¨˜éŒ²
        training_history['train_loss'].append(train_loss)
        training_history['val_loss'].append(val_loss)
        training_history['val_auc'].append(val_auc)
        training_history['learning_rate'].append(current_lr)
        
        # ãƒ­ã‚°å‡ºåŠ›
        print(f"Epoch {epoch+1}/{config['train']['epochs']}")
        print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, LR: {current_lr:.2e}")
        
        # W&B ãƒ­ã‚°
        wandb.log({
            f'fold_{fold}_train_loss': train_loss,
            f'fold_{fold}_val_loss': val_loss,
            f'fold_{fold}_val_auc': val_auc,
            'epoch': epoch,
            'learning_rate': current_lr
        })
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if val_auc > best_score:
            best_score = val_auc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_auc': val_auc,
                'val_predictions': val_preds
            }, f'model/fold_{fold}_best.pth')
            
            print(f"New best score: {val_auc:.4f}")
        
        # Early stopping ãƒã‚§ãƒƒã‚¯
        if early_stopping(val_auc, model):
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    return training_history, best_score
```

### 3.3 å­¦ç¿’å®Œäº†æ™‚ã®æˆæœç‰©ä¿å­˜

```python
def save_training_artifacts(fold_scores, oof_predictions, oof_targets, config, training_histories):
    """å­¦ç¿’æˆæœç‰©ã®ä¿å­˜"""
    
    # OOFäºˆæ¸¬ä¿å­˜
    oof_df = pd.DataFrame({
        'index': range(len(oof_targets)),
        'fold': oof_fold_assignments,
        'y_true': oof_targets,
        'y_pred': oof_predictions
    })
    oof_df.to_csv('oof_predictions.csv', index=False)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»ä¿å­˜
    cv_mean = np.mean(fold_scores)
    cv_std = np.std(fold_scores)
    oof_auc = roc_auc_score(oof_targets, oof_predictions)
    
    metrics = {
        'experiment_id': config['experiment']['id'],
        'cv_mean_auc': float(cv_mean),
        'cv_std_auc': float(cv_std),
        'oof_auc': float(oof_auc),
        'fold_scores': [float(x) for x in fold_scores],
        'n_folds': len(fold_scores),
        'training_completed': datetime.now().isoformat()
    }
    
    with open('metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    
    # å­¦ç¿’å±¥æ­´ä¿å­˜
    with open('training_history.json', 'w') as f:
        json.dump(training_histories, f, indent=2)
    
    # W&Bæœ€çµ‚ãƒ­ã‚°
    wandb.log({
        'final_cv_mean_auc': cv_mean,
        'final_cv_std_auc': cv_std,
        'final_oof_auc': oof_auc
    })
    
    print(f"Training completed! CV AUC: {cv_mean:.4f} Â± {cv_std:.4f}")
    return metrics
```

## 4ï¸âƒ£ OOFåˆ†æãƒ•ã‚§ãƒ¼ã‚ºï¼ˆevaluation.ipynbï¼‰

### 4.1 åŸºæœ¬æ€§èƒ½åˆ†æ

```python
def comprehensive_oof_analysis(oof_df, metrics):
    """åŒ…æ‹¬çš„ãªOOFåˆ†æ"""
    
    print("=" * 60)
    print("COMPREHENSIVE OOF ANALYSIS")
    print("=" * 60)
    
    # åŸºæœ¬çµ±è¨ˆ
    print(f"CV Mean AUC: {metrics['cv_mean_auc']:.6f} Â± {metrics['cv_std_auc']:.6f}")
    print(f"OOF AUC: {metrics['oof_auc']:.6f}")
    print(f"CV-OOF Diff: {abs(metrics['cv_mean_auc'] - metrics['oof_auc']):.6f}")
    
    # Foldåˆ¥åˆ†æ
    print(f"\nFold-wise Performance:")
    for i, score in enumerate(metrics['fold_scores']):
        print(f"  Fold {i+1}: {score:.6f}")
    
    # å“è³ªãƒã‚§ãƒƒã‚¯
    fold_std = metrics['cv_std_auc']
    cv_oof_diff = abs(metrics['cv_mean_auc'] - metrics['oof_auc'])
    
    # ã‚¢ãƒ©ãƒ¼ãƒˆ
    if fold_std > 0.02:
        print("âš ï¸  WARNING: High fold variance (potential instability)")
    if cv_oof_diff > 0.01:
        print("âš ï¸  WARNING: Large CV-OOF discrepancy")
    
    return {
        'fold_consistency': 1 - fold_std,
        'cv_oof_agreement': 1 - cv_oof_diff,
        'overall_quality': np.mean([1 - fold_std, 1 - cv_oof_diff])
    }
```

### 4.2 ãƒªãƒ¼ã‚¯æ¤œå‡ºãƒ»ç•°å¸¸å€¤åˆ†æ

```python
def detect_data_issues(oof_df, train_df):
    """ãƒ‡ãƒ¼ã‚¿å•é¡Œã®æ¤œå‡º"""
    
    issues_found = []
    
    # 1. ç•°å¸¸ã«é«˜ã„äºˆæ¸¬ç¢ºç‡
    extreme_high = (oof_df['y_pred'] > 0.99).sum()
    extreme_low = (oof_df['y_pred'] < 0.01).sum()
    extreme_ratio = (extreme_high + extreme_low) / len(oof_df)
    
    print(f"Extreme predictions: {extreme_ratio:.2%}")
    if extreme_ratio > 0.10:
        issues_found.append("Too many extreme predictions (potential overfitting)")
    
    # 2. Foldé–“ã®Targetåˆ†å¸ƒãƒã‚§ãƒƒã‚¯
    fold_target_rates = []
    for fold in oof_df['fold'].unique():
        fold_data = oof_df[oof_df['fold'] == fold]
        target_rate = fold_data['y_true'].mean()
        fold_target_rates.append(target_rate)
    
    target_rate_std = np.std(fold_target_rates)
    print(f"Fold target rate std: {target_rate_std:.4f}")
    if target_rate_std > 0.05:
        issues_found.append("Inconsistent target distribution across folds")
    
    # 3. äºˆæ¸¬åˆ†å¸ƒã®ãƒã‚§ãƒƒã‚¯
    neg_pred_mean = oof_df[oof_df['y_true'] == 0]['y_pred'].mean()
    pos_pred_mean = oof_df[oof_df['y_true'] == 1]['y_pred'].mean()
    separation = pos_pred_mean - neg_pred_mean
    
    print(f"Class separation: {separation:.4f}")
    if separation < 0.1:
        issues_found.append("Poor class separation in predictions")
    
    return issues_found

def analyze_prediction_patterns(oof_df):
    """äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    
    # ä¿¡é ¼åº¦åˆ¥åˆ†æ
    confidence_bins = pd.cut(oof_df['y_pred'], bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0], 
                           labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    
    confidence_analysis = oof_df.groupby(confidence_bins).agg({
        'y_true': ['count', 'mean'],
        'y_pred': 'mean'
    }).round(4)
    
    print("Prediction Confidence Analysis:")
    print(confidence_analysis)
    
    # Calibration åˆ†æ
    from sklearn.calibration import calibration_curve
    
    fraction_of_positives, mean_predicted_value = calibration_curve(
        oof_df['y_true'], oof_df['y_pred'], n_bins=10
    )
    
    calibration_error = np.mean(np.abs(fraction_of_positives - mean_predicted_value))
    print(f"\nCalibration Error: {calibration_error:.4f}")
    
    return confidence_analysis, calibration_error
```

### 4.3 é–¾å€¤æœ€é©åŒ–

```python
from scripts.metrics import ThresholdOptimizer

def optimize_classification_threshold(oof_df):
    """åˆ†é¡é–¾å€¤ã®æœ€é©åŒ–"""
    
    optimizer = ThresholdOptimizer(oof_df['y_true'], oof_df['y_pred'])
    
    # è¤‡æ•°æ‰‹æ³•ã§æœ€é©åŒ–
    methods = ['youden', 'f1', 'precision_recall_balance']
    threshold_results = []
    
    for method in methods:
        result = optimizer.optimize_threshold(method)
        
        # è©²å½“é–¾å€¤ã§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        y_pred_binary = (oof_df['y_pred'] >= result['threshold']).astype(int)
        
        metrics_at_threshold = {
            'method': method,
            'threshold': result['threshold'],
            'accuracy': accuracy_score(oof_df['y_true'], y_pred_binary),
            'f1': f1_score(oof_df['y_true'], y_pred_binary),
            'precision': precision_score(oof_df['y_true'], y_pred_binary),
            'recall': recall_score(oof_df['y_true'], y_pred_binary)
        }
        
        threshold_results.append(metrics_at_threshold)
    
    threshold_df = pd.DataFrame(threshold_results)
    
    # æ¨å¥¨é–¾å€¤ã®æ±ºå®šï¼ˆYoudenæ³•ã‚’å„ªå…ˆï¼‰
    recommended_threshold = threshold_df[threshold_df['method'] == 'youden']['threshold'].iloc[0]
    
    # æœ€é©åŒ–çµæœã‚’ä¿å­˜
    threshold_optimization = {
        'methods_comparison': threshold_df.to_dict('records'),
        'recommended_threshold': float(recommended_threshold),
        'optimization_date': datetime.now().isoformat()
    }
    
    with open('threshold_optimization.json', 'w') as f:
        json.dump(threshold_optimization, f, indent=2)
    
    return threshold_df, recommended_threshold
```

## 5ï¸âƒ£ æ¨è«–ãƒ»æå‡ºãƒ•ã‚§ãƒ¼ã‚ºï¼ˆinference.ipynbï¼‰

### 5.1 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¨è«–

```python
def inference_with_tta(models, test_loader, tta_transforms, device):
    """TTAä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è«–"""
    
    all_predictions = []
    
    # TTAåˆ¥ã«æ¨è«–
    for tta_name, transform in tta_transforms.items():
        print(f"Running inference with TTA: {tta_name}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å†æ§‹ç¯‰
        test_dataset.transform = transform
        current_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        tta_predictions = []
        
        for model in models:
            model.eval()
            fold_predictions = []
            
            with torch.no_grad():
                for images in tqdm(current_loader):
                    images = images.to(device)
                    
                    with torch.cuda.amp.autocast():
                        outputs = model(images)
                        probs = torch.sigmoid(outputs).cpu().numpy()
                    
                    fold_predictions.extend(probs.flatten())
            
            tta_predictions.append(fold_predictions)
        
        # ãƒ¢ãƒ‡ãƒ«é–“ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_predictions = np.mean(tta_predictions, axis=0)
        all_predictions.append(ensemble_predictions)
    
    # TTAé–“ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    final_predictions = np.mean(all_predictions, axis=0)
    
    return final_predictions

def create_submission_with_metadata(test_df, predictions, recommended_threshold, config):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ç¢ºç‡ç‰ˆ
    submission_proba = test_df[['image_id']].copy()
    submission_proba['aneurysm'] = predictions
    proba_filename = f"submissions/{config['experiment']['id']}_proba_{timestamp}.csv"
    submission_proba.to_csv(proba_filename, index=False)
    
    # äºŒå€¤ç‰ˆ
    submission_binary = test_df[['image_id']].copy()
    submission_binary['aneurysm'] = (predictions >= recommended_threshold).astype(int)
    binary_filename = f"submissions/{config['experiment']['id']}_binary_th{recommended_threshold:.3f}_{timestamp}.csv"
    submission_binary.to_csv(binary_filename, index=False)
    
    # æå‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    submission_metadata = {
        'experiment_id': config['experiment']['id'],
        'submission_time': datetime.now().isoformat(),
        'git_sha': get_git_sha(),
        'model_files': [f"model/fold_{i}_best.pth" for i in range(5)],
        'tta_enabled': True,
        'recommended_threshold': float(recommended_threshold),
        'test_samples': len(test_df),
        'predicted_positive': int((predictions >= recommended_threshold).sum()),
        'predicted_positive_rate': float((predictions >= recommended_threshold).mean()),
        'files': {
            'probability': proba_filename,
            'binary': binary_filename
        }
    }
    
    with open('submission_metadata.json', 'w') as f:
        json.dump(submission_metadata, f, indent=2)
    
    return proba_filename, binary_filename, submission_metadata
```

### 5.2 è‡ªå‹•Kaggleæå‡º

```python
def submit_to_kaggle_with_retry(submission_file, config, max_retries=3):
    """å†è©¦è¡Œæ©Ÿèƒ½ä»˜ãKaggleæå‡º"""
    
    git_sha = get_git_sha()[:8]
    message = config['kaggle']['submission_message'].format(git_sha=git_sha)
    
    for attempt in range(max_retries):
        try:
            cmd = [
                'kaggle', 'competitions', 'submit',
                '-c', config['kaggle']['competition'],
                '-f', submission_file,
                '-m', message
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            print(f"âœ… Successfully submitted to Kaggle!")
            print(f"Output: {result.stdout}")
            
            # æå‡ºæˆåŠŸã‚’ãƒ­ã‚°
            wandb.log({
                'kaggle_submission_success': 1,
                'submission_attempt': attempt + 1
            })
            
            return True, result.stdout
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Submission attempt {attempt + 1} failed: {e.stderr}")
            
            if attempt < max_retries - 1:
                print("Retrying in 10 seconds...")
                time.sleep(10)
            else:
                print("Manual submission required:")
                print(f"kaggle competitions submit -c {config['kaggle']['competition']} -f {submission_file} -m '{message}'")
                
                # æå‡ºå¤±æ•—ã‚’ãƒ­ã‚°
                wandb.log({
                    'kaggle_submission_success': 0,
                    'submission_attempts': max_retries
                })
                
                return False, e.stderr
    
    return False, "Max retries exceeded"
```

## 6ï¸âƒ£ çµæœè¨˜éŒ²ãƒ»æ¬¡å®Ÿé¨“è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º

### 6.1 å®Ÿé¨“å°å¸³ã®æ›´æ–°

```python
def update_experiments_log(experiment_results, lb_score=None):
    """å®Ÿé¨“å°å¸³ã¸ã®çµæœè¿½è¨˜"""
    
    experiments_file = "../../experiments.csv"
    
    # å®Ÿé¨“çµæœè¡Œã®ä½œæˆ
    experiment_row = {
        'exp_id': experiment_results['experiment_id'],
        'date': datetime.now().strftime('%Y-%m-%d'),
        'description': experiment_results.get('description', ''),
        'hypothesis': experiment_results.get('hypothesis', ''),
        'cv_mean_auc': experiment_results['cv_mean_auc'],
        'cv_std_auc': experiment_results['cv_std_auc'],
        'oof_auc': experiment_results['oof_auc'],
        'lb_public': lb_score if lb_score else None,
        'lb_private': None,  # ã‚³ãƒ³ãƒšçµ‚äº†å¾Œã«æ›´æ–°
        'fold_consistency': experiment_results.get('fold_consistency', None),
        'cv_reliability_score': experiment_results.get('cv_reliability_score', None),
        'git_sha': get_git_sha()[:8],
        'wandb_url': experiment_results.get('wandb_url', ''),
        'model_architecture': experiment_results.get('model_architecture', ''),
        'key_changes': experiment_results.get('key_changes', ''),
        'success': experiment_results['cv_mean_auc'] > 0.80,  # åŸºæº–ã‚¹ã‚³ã‚¢
        'notes': ''
    }
    
    # CSVæ›´æ–°
    if os.path.exists(experiments_file):
        experiments_df = pd.read_csv(experiments_file)
    else:
        experiments_df = pd.DataFrame()
    
    new_row_df = pd.DataFrame([experiment_row])
    experiments_df = pd.concat([experiments_df, new_row_df], ignore_index=True)
    experiments_df.to_csv(experiments_file, index=False)
    
    print(f"Experiments log updated: {experiments_file}")
    return experiments_df
```

### 6.2 è©³ç´°ãªå®Ÿé¨“ãƒãƒ¼ãƒˆä½œæˆ

```python
def generate_experiment_notes(experiment_results, threshold_results, data_quality_issues):
    """è©³ç´°å®Ÿé¨“ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ"""
    
    notes_template = f"""
# å®Ÿé¨“ãƒãƒ¼ãƒˆ - {experiment_results['experiment_id']}

## ğŸ“Š å®Ÿé¨“ã‚µãƒãƒªãƒ¼
- **å®Ÿé¨“æ—¥**: {datetime.now().strftime('%Y-%m-%d')}
- **ä»®èª¬**: {experiment_results.get('hypothesis', 'N/A')}
- **Git SHA**: {get_git_sha()[:8]}
- **W&B Run**: {experiment_results.get('wandb_url', 'N/A')}

## ğŸ“ˆ æ€§èƒ½çµæœ
- **CV AUC**: {experiment_results['cv_mean_auc']:.4f} Â± {experiment_results['cv_std_auc']:.4f}
- **OOF AUC**: {experiment_results['oof_auc']:.4f}
- **æœ€é©é–¾å€¤**: {threshold_results['recommended_threshold']:.4f} (Youden method)

### Foldåˆ¥æ€§èƒ½
"""
    
    for i, score in enumerate(experiment_results['fold_scores']):
        notes_template += f"- Fold {i+1}: {score:.4f}\n"
    
    notes_template += f"""
## ğŸ” å“è³ªåˆ†æ

### ãƒ‡ãƒ¼ã‚¿å“è³ª
"""
    
    if data_quality_issues:
        for issue in data_quality_issues:
            notes_template += f"- âš ï¸ {issue}\n"
    else:
        notes_template += "- âœ… No significant data quality issues detected\n"
    
    notes_template += f"""

### ãƒ¢ãƒ‡ãƒ«ä¿¡é ¼æ€§
- **Foldä¸€è²«æ€§**: {experiment_results.get('fold_consistency', 'N/A')}
- **CV-OOFä¸€è‡´æ€§**: {experiment_results.get('cv_oof_agreement', 'N/A')}
- **ç·åˆå“è³ªã‚¹ã‚³ã‚¢**: {experiment_results.get('cv_reliability_score', 'N/A')}

## ğŸ¯ ä¸»è¦ãªç™ºè¦‹

### æˆåŠŸç‚¹
- [å®Ÿéš›ã®çµæœã«åŸºã¥ã„ã¦æ‰‹å‹•ã§è¨˜å…¥]

### èª²é¡Œç‚¹
- [å®Ÿéš›ã®çµæœã«åŸºã¥ã„ã¦æ‰‹å‹•ã§è¨˜å…¥]

## ğŸ”® æ¬¡å®Ÿé¨“ã¸ã®ç¤ºå”†

### æ”¹å–„ã‚¢ã‚¤ãƒ‡ã‚¢
1. [å®Ÿéš›ã®åˆ†æçµæœã«åŸºã¥ã„ã¦è¨˜å…¥]
2. [å®Ÿéš›ã®åˆ†æçµæœã«åŸºã¥ã„ã¦è¨˜å…¥]
3. [å®Ÿéš›ã®åˆ†æçµæœã«åŸºã¥ã„ã¦è¨˜å…¥]

### ãƒªã‚¹ã‚¯è¦å› 
- [ç‰¹å®šã•ã‚ŒãŸå•é¡Œç‚¹ã‚’è¨˜å…¥]

## ğŸ“‹ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
- [ ] [å…·ä½“çš„ãªæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—]
- [ ] [å…·ä½“çš„ãªæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—]
- [ ] [å…·ä½“çš„ãªæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—]

---
*ã“ã®ãƒãƒ¼ãƒˆã¯å®Ÿé¨“å®Œäº†æ™‚ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open('detailed_experiment_notes.md', 'w', encoding='utf-8') as f:
        f.write(notes_template)
    
    print("Detailed experiment notes saved: detailed_experiment_notes.md")
    return notes_template
```

## ğŸ”„ ç¶™ç¶šçš„æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«

### CV-LBç›¸é–¢ã®è¿½è·¡

```python
def analyze_cv_lb_correlation(experiments_df):
    """CV-LBç›¸é–¢åˆ†æ"""
    
    # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
    valid_experiments = experiments_df.dropna(subset=['cv_mean_auc', 'lb_public'])
    
    if len(valid_experiments) < 3:
        print("Insufficient data for correlation analysis")
        return None
    
    # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
    correlation = valid_experiments['cv_mean_auc'].corr(valid_experiments['lb_public'])
    
    print(f"CV-LB Correlation: {correlation:.4f}")
    
    # ç›¸é–¢ã®å“è³ªåˆ¤å®š
    if correlation > 0.8:
        print("âœ… Excellent correlation - Trust your CV!")
    elif correlation > 0.6:
        print("âœ… Good correlation - CV is reliable")
    elif correlation > 0.4:
        print("âš ï¸ Fair correlation - Some caution needed")
    else:
        print("âŒ Poor correlation - Review CV strategy")
    
    # æ•£å¸ƒå›³ä½œæˆ
    plt.figure(figsize=(10, 6))
    plt.scatter(valid_experiments['cv_mean_auc'], valid_experiments['lb_public'], 
               alpha=0.7, s=100)
    
    for _, row in valid_experiments.iterrows():
        plt.annotate(row['exp_id'], 
                    (row['cv_mean_auc'], row['lb_public']),
                    xytext=(5, 5), textcoords='offset points')
    
    plt.xlabel('CV AUC')
    plt.ylabel('LB Public AUC')
    plt.title(f'CV-LB Correlation (r={correlation:.4f})')
    plt.grid(True, alpha=0.3)
    
    # å›å¸°ç·š
    z = np.polyfit(valid_experiments['cv_mean_auc'], valid_experiments['lb_public'], 1)
    p = np.poly1d(z)
    plt.plot(valid_experiments['cv_mean_auc'], p(valid_experiments['cv_mean_auc']), "r--", alpha=0.8)
    
    plt.tight_layout()
    plt.savefig('cv_lb_correlation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return correlation
```

### è‡ªå‹•å®Ÿé¨“ææ¡ˆã‚·ã‚¹ãƒ†ãƒ 

```python
def suggest_next_experiments(experiments_df, current_best_score):
    """æ¬¡å®Ÿé¨“ã®è‡ªå‹•ææ¡ˆ"""
    
    suggestions = []
    
    # åŸºæœ¬çš„ãªæ”¹å–„ææ¡ˆ
    if current_best_score < 0.85:
        suggestions.extend([
            "Try different model architectures (EfficientNet, Vision Transformer)",
            "Experiment with advanced data augmentation",
            "Implement multi-scale training",
            "Add external data or pseudo-labeling"
        ])
    elif current_best_score < 0.90:
        suggestions.extend([
            "Ensemble multiple models",
            "Fine-tune with different learning rates",
            "Implement advanced techniques (mixup, cutmix)",
            "Optimize post-processing and thresholding"
        ])
    else:
        suggestions.extend([
            "Focus on model ensemble strategies", 
            "Implement test-time augmentation optimization",
            "Fine-tune submission strategy",
            "Investigate stacking approaches"
        ])
    
    # CV-LBç›¸é–¢ã«åŸºã¥ãææ¡ˆ
    if len(experiments_df) > 2:
        correlation = analyze_cv_lb_correlation(experiments_df)
        if correlation is not None and correlation < 0.7:
            suggestions.append("Review cross-validation strategy due to low CV-LB correlation")
    
    # æœ€è¿‘ã®å‚¾å‘ã«åŸºã¥ãææ¡ˆ
    recent_experiments = experiments_df.tail(3)
    if len(recent_experiments) > 1:
        recent_improvement = recent_experiments['cv_mean_auc'].diff().mean()
        if recent_improvement < 0.001:
            suggestions.append("Consider more significant changes - recent improvements are minimal")
    
    print("ğŸ”® Next Experiment Suggestions:")
    for i, suggestion in enumerate(suggestions[:5], 1):  # ä¸Šä½5ã¤
        print(f"  {i}. {suggestion}")
    
    return suggestions
```

## ğŸ“‹ å®Ÿé¨“ç®¡ç†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿé¨“é–‹å§‹æ™‚
- [ ] æ˜ç¢ºãªä»®èª¬ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹
- [ ] å¤‰æ›´ç‚¹ãŒå‰å®Ÿé¨“ã‹ã‚‰æ˜ç¢ºã«åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹
- [ ] CVåˆ†å‰²ãŒé©åˆ‡ã«å›ºå®šã•ã‚Œã¦ã„ã‚‹
- [ ] Git commitãŒå®Œäº†ã—ã¦ã„ã‚‹

### å­¦ç¿’å®Ÿè¡Œä¸­
- [ ] W&Bãƒ­ã‚°ãŒæ­£å¸¸ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹
- [ ] GPU/CPUãƒªã‚½ãƒ¼ã‚¹ãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹
- [ ] Early stoppingãŒé©åˆ‡ã«å‹•ä½œã—ã¦ã„ã‚‹
- [ ] ä¸­é–“çµæœãŒå®šæœŸçš„ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹

### è©•ä¾¡åˆ†ææ™‚
- [ ] OOFåˆ†æãŒå®Œäº†ã—ã¦ã„ã‚‹
- [ ] ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡ŒãŒæ¤œå‡ºãƒ»è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹
- [ ] é–¾å€¤æœ€é©åŒ–ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹
- [ ] CV-LBç›¸é–¢ãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹

### å®Ÿé¨“å®Œäº†æ™‚
- [ ] ã™ã¹ã¦ã®æˆæœç‰©ãŒé©åˆ‡ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
- [ ] å®Ÿé¨“å°å¸³ãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹
- [ ] è©³ç´°ãƒãƒ¼ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã‚‹
- [ ] æ¬¡å®Ÿé¨“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ãŒæ•´ç†ã•ã‚Œã¦ã„ã‚‹

---

ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¾“ã†ã“ã¨ã§ã€ä½“ç³»çš„ã‹ã¤åŠ¹ç‡çš„ãªå®Ÿé¨“ç®¡ç†ã‚’å®Ÿç¾ã—ã€æœ€é«˜å“è³ªã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚